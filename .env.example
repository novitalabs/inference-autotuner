# LLM Inference Autotuner Configuration
# Copy this file to .env and customize as needed

# Database
# DATABASE_URL=sqlite+aiosqlite:///~/.local/share/inference-autotuner/autotuner.db

# Redis (for ARQ worker)
# REDIS_HOST=localhost
# REDIS_PORT=6379
# REDIS_DB=0

# Docker Model Path
# DOCKER_MODEL_PATH=/mnt/data/models

# Deployment Mode
# DEPLOYMENT_MODE=docker

# Proxy Settings (optional - for Docker containers to access external networks)
# If your environment requires a proxy to access HuggingFace or other external services,
# configure these settings. They will be passed to Docker containers as environment variables.

# HTTP_PROXY=http://proxy.example.com:8080
# HTTPS_PROXY=http://proxy.example.com:8080
# NO_PROXY=localhost,127.0.0.1,.local

# Example with authentication:
# HTTP_PROXY=http://username:password@proxy.example.com:8080
# HTTPS_PROXY=http://username:password@proxy.example.com:8080

# HuggingFace Token (optional - required for gated models like Llama)
# Get your token from https://huggingface.co/settings/tokens
# This token will be passed to Docker containers to download gated models
# HF_TOKEN=hf_your_token_here
