# Development Diary - 2025-12-11

## 1. GitHub Issue #1 Fix: HuggingFace Offline Mode Support

> Check Task 25 failure reason, can test related issues directly via command line

<details>
<summary>Agent Response</summary>

### Problem Analysis

Task 25 failed with error: `[Errno 101] Network is unreachable` when connecting to huggingface.co

**Root Cause Chain:**
1. genai-bench needs to load tokenizer for metrics calculation
2. Even though HF cache EXISTS (`models--meta-llama--Llama-3.2-1B-Instruct`), the library tries to verify with HuggingFace API
3. No proxy configured in environment, direct connection to huggingface.co fails
4. `HTTPS_PROXY` not set in subprocess environment
5. `_ensure_tokenizer_cached()` was called but didn't enable offline mode

### Verification Steps

```bash
# 1. Confirmed HF cache exists with tokenizer_config.json
ls ~/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/*/
# Output: config.json, generation_config.json, model.safetensors,
#         special_tokens_map.json, tokenizer_config.json, tokenizer.json

# 2. Confirmed network issue
curl -I https://huggingface.co  # FAILS - Network unreachable
curl -x http://172.17.0.1:1081 -I https://huggingface.co  # WORKS

# 3. Confirmed offline mode can work with direct path
HF_HUB_OFFLINE=1 python3 -c "
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('/path/to/cache/snapshot')
"  # SUCCESS
```

### Solution Implementation

**1. Modified `direct_benchmark_controller.py`:**

Changed `_ensure_tokenizer_cached()` to return tuple `(success, is_fully_cached)`:
- Checks if `tokenizer_config.json` exists in cache snapshots
- Returns `True, True` if fully cached (can use offline mode)
- Returns `True, False` if partially cached (needs online access)

Added proxy fallback from config and `HF_HUB_OFFLINE` environment variable:
```python
# Get proxy from environment or config
from web.config import get_settings
settings = get_settings()
proxy_url = os.environ.get('HTTPS_PROXY') or settings.https_proxy

# Set offline mode if tokenizer fully cached
if use_offline_mode:
    env['HF_HUB_OFFLINE'] = '1'
```

**2. Modified `genai-bench/cli/validation.py`:**

Enhanced `validate_tokenizer()` to support offline mode:
```python
local_files_only = os.environ.get("HF_HUB_OFFLINE", "0") == "1"

if local_files_only:
    # Resolve to local cache path first
    cache_dir = Path.home() / ".cache" / "huggingface" / "hub"
    cache_name = "models--" + model_tokenizer.replace("/", "--")
    cached_path = cache_dir / cache_name / "snapshots"
    if cached_path.exists():
        snapshot_dirs = list(cached_path.iterdir())
        if snapshot_dirs:
            return AutoTokenizer.from_pretrained(snapshot_dirs[0])
```

### Test Results

```bash
# Before fix: Network timeout
HF_HUB_OFFLINE=1 genai-bench benchmark --model-tokenizer meta-llama/Llama-3.2-1B-Instruct ...
# Result: Hung for 10+ seconds trying to reach HuggingFace

# After fix: Loads from cache
INFO     Offline mode: loading tokenizer from cache:
         /home/claude/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/9213176726f574b556790deb65791e0c5aa438b6
INFO     The average character token ratio is: 4.0602006688963215
# Success in <1 second
```

### Files Modified

1. `src/controllers/direct_benchmark_controller.py`:
   - `_ensure_tokenizer_cached()` now returns `tuple[bool, bool]`
   - Added proxy fallback from `web.config.get_settings()`
   - Added `HF_HUB_OFFLINE=1` when tokenizer is fully cached

2. `third_party/genai-bench/genai_bench/cli/validation.py`:
   - Added offline mode support to `validate_tokenizer()`
   - Resolves model ID to local cache path when `HF_HUB_OFFLINE=1`

### Impact

- Fixes Task 25 and similar failures where tokenizer is cached but network unreachable
- No longer requires proxy configuration if tokenizer already downloaded
- Faster benchmark startup (skips HuggingFace API verification)
- Still downloads tokenizer if not cached (using proxy if configured)

</details>

---

## 2. Confirm start_worker.sh Environment Variable Loading

> Ensure start_worker.sh loads necessary environment variables from .env, including HF_TOKEN and proxy settings

<details>
<summary>Agent Response</summary>

### Verification

`scripts/start_worker.sh` already correctly loads environment variables from `.env` (lines 27-44):

```bash
# Load environment variables from .env if it exists
if [ -f "$PROJECT_ROOT/.env" ]; then
    echo "Loading environment variables from .env..."
    set -a
    source "$PROJECT_ROOT/.env"
    set +a
fi
```

### Worker Startup Test

```bash
./scripts/start_worker.sh
# Output:
# Checking for existing ARQ worker...
# Loading environment variables from .env...
# HTTP_PROXY configured: http://172.17.0.1:1081
# HTTPS_PROXY configured: http://172.17.0.1:1081
# NO_PROXY configured: localhost,127.0.0.1
# Starting ARQ worker with nohup...
# ARQ worker started with PID: 3474283
```

### Environment Variables Verified in Worker Process

```bash
cat /proc/$WORKER_PID/environ | tr '\0' '\n' | grep -E "^(HF_TOKEN|HTTP_PROXY|HTTPS_PROXY|NO_PROXY)="
# Output:
# HF_TOKEN=hf_...
# NO_PROXY=localhost,127.0.0.1
# HTTPS_PROXY=http://172.17.0.1:1081
# HTTP_PROXY=http://172.17.0.1:1081
```

### `.env` Configuration

```env
# Proxy Configuration
HTTP_PROXY=http://172.17.0.1:1081
HTTPS_PROXY=http://172.17.0.1:1081
NO_PROXY=localhost,127.0.0.1

# HuggingFace Token
HF_TOKEN=hf_xxx...
```

### Conclusion

- `start_worker.sh` is correctly configured
- All necessary environment variables (HF_TOKEN, HTTP_PROXY, HTTPS_PROXY, NO_PROXY) are loaded
- Every time using `./scripts/start_worker.sh` to start the worker, these variables are automatically loaded

</details>

---

## 3. Task 25 Retry and Success Verification

> Check Task 25, why did it fail again?

<details>
<summary>Agent Response</summary>

### Problem Analysis

Task 25 initially failed due to two issues:

1. **Permission Issue**: `PermissionError: [Errno 13] Permission denied: '/home/claude/.local/share/inference-autotuner/logs/task_25_exp_1.log'`
   - Cause: Previous log files were created by root, but worker was running as claude user
   - Fix: `sudo chown -R claude:claude ~/.local/share/inference-autotuner/logs/`

2. **Database Enum Value Error**: `LookupError: 'pending' is not among the defined enum values. Enum name: taskstatus. Possible values: PENDING, RUNNING, COMPLETED, ..., CANCELLED`
   - Cause: Manual SQL update used lowercase `pending` instead of uppercase `PENDING`
   - Fix: Use correct enum value `UPDATE tasks SET status = 'PENDING' WHERE id = 25`

### Retry Results

After fixes, restarted Task 25, all 4 experiments completed successfully:

```
[11:02:02] Status: completed, Progress: 4/4
Task finished with status: completed
```

### Final Results

```
Task 25: llama3.2-1b-stable-tuning-v2
Status: completed
Successful experiments: 4/4 (100%)
Best experiment ID: 206
Runtime: 581.80s (~9.7 minutes)
Generation throughput: ~333 tokens/s
```

### Key Logs Confirming Fix

```
[Tokenizer] Fully cached (offline mode OK): meta-llama/Llama-3.2-1B-Instruct
[Benchmark] HF_HUB_OFFLINE=1 (tokenizer fully cached, skipping HuggingFace API)
INFO     Offline mode: loading tokenizer from cache:
         /home/claude/.cache/huggingface/hub/models--meta-llama--Llama-3.2-1B-Instruct/snapshots/...
```

### Fix Summary

| Issue | Solution |
|-------|----------|
| Log file permissions | `chown -R claude:claude logs/` |
| Database enum value | Use uppercase `PENDING` instead of `pending` |
| HuggingFace network issue | `HF_HUB_OFFLINE=1` offline mode |

</details>

---

## 4. Update genai-bench Submodule to Fork

> Update submodule genai-bench URL to https://github.com/k-l-lambda/genai-bench.git, develop branch

<details>
<summary>Agent Response</summary>

### Changes Made

Updated `.gitmodules` to point to the fork repository:

```ini
[submodule "third_party/genai-bench"]
    path = third_party/genai-bench
    url = https://github.com/k-l-lambda/genai-bench.git
    branch = develop
```

### Sync Commands

```bash
git submodule sync
cd third_party/genai-bench
git fetch origin
git checkout origin/develop
```

### Verification

The fork's develop branch (commit 7466eb6) already contains the HF_HUB_OFFLINE support fix, so changes to `validation.py` in our local copy are now synchronized with upstream fork.

</details>

---

## 5. Fix GitHub Actions Calendar Workflow Failures

> Check latest workflow run errors, try to fix

<details>
<summary>Agent Response</summary>

### Problem Analysis

The "Calendar build" workflow had been failing for 7 consecutive days on the main branch.

```bash
gh run list --repo novitalabs/inference-autotuner --workflow=calendar.yml --limit 5
# All showing: completed failure Calendar build main schedule
```

### Root Cause

The workflow uses `schedule` trigger which **always runs on the default branch (main)**. The main branch had an older version of the workflow using `actions-js/push@master`, which doesn't handle the case where remote has new commits.

Error from logs:
```
! [rejected]        HEAD -> develop (fetch first)
error: failed to push some refs to 'https://github.com/novitalabs/inference-autotuner.git'
hint: Updates were rejected because the remote contains work that you do not have locally.
```

### Solution

The develop branch already had a fixed version using `git pull --rebase` before push. The fix was to merge the develop branch's workflow file to main branch.

Key difference:
- **Old (main)**: Used `actions-js/push@master` - doesn't handle conflicts
- **New (develop)**: Uses native git commands with rebase:
  ```yaml
  - name: Commit and push changes
    run: |
      git config --local user.email "github-actions[bot]@users.noreply.github.com"
      git config --local user.name "github-actions[bot]"
      git add -A
      git diff --staged --quiet || git commit -m "chore: update calendar [skip ci]"
      git pull --rebase origin develop
      git push origin develop
  ```

### Manual Trigger Test

After user merged the fix to main, added `workflow_dispatch` trigger and tested:

```bash
gh workflow run calendar.yml --repo novitalabs/inference-autotuner --ref main
gh run list --repo novitalabs/inference-autotuner --workflow=calendar.yml --limit 3

# Result:
# completed success trigger workflow. Calendar build main workflow_dispatch
```

### Summary

| Issue | Cause | Solution |
|-------|-------|----------|
| Calendar workflow failing | `schedule` runs on main branch with old workflow | Merge develop's fixed workflow to main |
| Cannot manually trigger | No `workflow_dispatch` trigger | Added `workflow_dispatch:` to workflow |

</details>
