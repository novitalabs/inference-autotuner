
> Fixed InferenceService template parameter issue and verified deployment

<details>
<summary>InferenceService Template Parameter Fix - Corrected params dictionary passing</summary>

* *User*: Continue the ome task development, append ClusterBaseModel and ClusterServingRuntime for a new created task.

* *Claude*: 
  **Issue Discovered**: When testing task 9 (`ome-cbm-csr-verified`), the InferenceService was created but never became ready after 540+ seconds. Investigation revealed the InferenceService had empty parameter values (`--tp-size=` with no value), causing deployment failure.

  **Root Cause Analysis**:
  1. Template used `params['tp-size']` to access parameters
  2. Controller passed parameters via `**parameters` unpacking (e.g., `tp_size=1`)
  3. Result: Jinja2 error `'params' is undefined` on subsequent tasks

  **Files Read**:
  - `src/templates/inference_service.yaml.j2:27-35` - Template with bracket notation
  - `src/config/clusterbasemodel_presets.py` - Model presets with metadata
  - `src/templates/clusterbasemodel.yaml.j2` - CBM template structure
  - Task logs showing deployment steps and timeout

  **Fix Applied**:
  Changed `src/controllers/ome_controller.py:105`:
  ```python
  # Before (incorrect):
  rendered = self.isvc_template.render(..., **parameters)
  
  # After (correct):
  rendered = self.isvc_template.render(..., params=parameters)
  ```

  **Verification**:
  1. Created test script `/tmp/test_template.py` to validate template rendering
  2. Confirmed correct output:
     ```yaml
     runner:
       args:
         - --tp-size=1  â† Has value (not empty!)
       resources:
         limits:
           nvidia.com/gpu: 1
     ```
  3. Restarted ARQ worker to pick up changes
  4. Created task 11 (`template-fix-verified`) and deployed successfully
  5. Verified InferenceService YAML in cluster had correct parameter values

  **Results**:
  - âœ… Task 10 had failed with `'params' is undefined` error
  - âœ… Task 11 successfully created InferenceService with proper parameters
  - âœ… No template errors in deployment (Steps 0a, 0b, 1 completed)
  - âœ… OME controller accepted and is reconciling the InferenceService

  **OME Infrastructure Issue Discovered**:
  While our template fix is working correctly, discovered separate infrastructure issue:
  - `ome-model-agent-daemonset` is crashing with: `"couldn't find shape in the shape mapping"`
  - Root cause: Missing `ome.io/shape` label on GPU nodes
  - Attempted fixes: Added labels `h20.8` and `VM.GPU.A10.1` but model-agent still fails
  - Impact: InferenceService cannot start pods without functional model-agent
  - This is an OME operational issue, not related to autotuner code

  **Summary**:
  - Template parameter fix: **VERIFIED AND WORKING**
  - InferenceService creation: **SUCCESS**
  - Pod scheduling: **BLOCKED** by OME model-agent infrastructure issue
  - Next step: Fix OME model-agent shape configuration (separate from autotuner)

</details>

---

> It seems APIs calling for dashboard not use the proxy port now.

<details>
<summary>Fixed hardcoded API URLs that were bypassing the proxy</summary>

**Issue**: Dashboard and other API services were using hardcoded `http://localhost:8000` URLs instead of the proxy configuration.

**Root Cause**: 
- `.env` was configured correctly with `VITE_API_URL=/api`
- Vite proxy was configured correctly
- BUT several service files had hardcoded `API_BASE_URL` constants that bypassed the configuration

**Files Fixed**:
1. **`src/services/dashboardApi.ts`**: 
   - Changed from `const API_BASE_URL = 'http://localhost:8000'`
   - Changed to `const API_BASE_URL = import.meta.env.VITE_API_URL || '/api'`
   - Removed duplicate `/api` prefix in endpoint paths

2. **`src/services/presetService.ts`**:
   - Same fix as above
   - Now respects environment variable and uses proxy

3. **`src/services/runtimeParamsService.ts`**:
   - Same fix as above
   - All endpoints now use relative paths through proxy

**Pattern Applied**:
```typescript
// Before (hardcoded):
const API_BASE = 'http://localhost:8000';
const response = await axios.get(`${API_BASE}/api/dashboard/gpu-status`);

// After (proxy-aware):
const API_BASE = import.meta.env.VITE_API_URL || '/api';
const response = await axios.get(`${API_BASE}/dashboard/gpu-status`);
```

**Why This Matters**:
- Hardcoded URLs cause CORS issues in development
- Bypasses the Vite proxy configuration
- Makes production deployment more difficult
- Inconsistent with the main API client (`api.ts`)

**Verification**:
- Searched all `.ts` and `.tsx` files for `localhost:8000`
- Only comment references remain (in useWebSocket.ts JSDoc)
- All API services now use environment-based configuration

**Next Step**: Restart frontend dev server for changes to take effect
```bash
# Kill existing dev server
pkill -f "vite"

# Restart
cd frontend && npm run dev
```

</details>

---

> Extend timeout for an experiment to 1800s, and start new experiment after stop previous container.

<details>
<summary>Extended timeout to 1800s and added explicit container cleanup on timeout</summary>

**User Request**: Increase experiment timeout to 1800 seconds (30 minutes) and ensure containers are stopped before starting the next experiment.

**Root Cause Analysis from Task 25**:
- Experiment 1 timed out after 600 seconds during benchmarking
- The benchmark was running with 6 concurrency levels and large token counts (3000 input + 300 output)
- After timeout, experiments 2-10 failed with "0 GPUs available"
- **Issue**: When timeout occurred, the container from experiment 1 wasn't properly cleaned up, causing GPU allocation to fail for subsequent experiments

**Changes Made**:

1. **Extended Default Timeout** (`src/web/workers/autotuner_worker.py:271`):
   ```python
   # Before:
   timeout_per_iteration = optimization_config.get("timeout_per_iteration", 600)  # Default 10 minutes
   
   # After:
   timeout_per_iteration = optimization_config.get("timeout_per_iteration", 1800)  # Default 30 minutes
   ```

2. **Added Explicit Container Cleanup on Timeout** (`src/web/workers/autotuner_worker.py:577-587`):
   ```python
   # After timeout error:
   # CRITICAL: Force cleanup of stalled container
   task_name = task.task_name
   namespace = task.model_config.get("namespace", "default")
   service_id = f"{task_name}-exp{iteration}"
   logger.info(f"[Cleanup] Forcing cleanup of service '{service_id}' after timeout")
   try:
       loop = asyncio.get_event_loop()
       await loop.run_in_executor(None, orchestrator.cleanup_experiment, service_id, None, namespace, iteration)
       logger.info(f"[Cleanup] Successfully cleaned up service '{service_id}'")
   except Exception as cleanup_error:
       logger.error(f"[Cleanup] Failed to cleanup service: {cleanup_error}")
   ```

**How It Works**:
- When `asyncio.TimeoutError` is raised, the code now explicitly calls `orchestrator.cleanup_experiment()`
- This ensures the container is stopped and removed before moving to the next experiment
- GPUs are released back to the pool, making them available for subsequent experiments
- The cleanup runs synchronously in an executor to ensure it completes before continuing

**Benefits**:
1. **Longer timeout** accommodates:
   - Large model initialization (~3-5 minutes)
   - Multiple concurrency levels
   - Large token counts (3000+ input tokens)
   - Complex benchmarks with 50+ requests per level

2. **Guaranteed cleanup** ensures:
   - Containers are always stopped after timeout
   - GPUs are released for next experiment
   - No resource leaks
   - Subsequent experiments can proceed

**Testing**:
- Restarted ARQ worker to load changes
- Worker process confirmed running (PID: 4119671)

**Files Modified**:
- `src/web/workers/autotuner_worker.py` (2 changes)

**Next Steps for Users**:
- Default timeout is now 30 minutes
- Can still override per-task: `"optimization": {"timeout_per_iteration": 3600}`
- For very large models or benchmarks, consider 60+ minutes

</details>

---

> Clean up and restart task 26

<details>
<summary>Task 26 cleanup and Task 27 creation with reduced concurrency</summary>

**Issue**: Task 26 experiments failing with "Failed to retrieve benchmark results" due to HTTP 503 errors at high concurrency levels (24, 32).

**Root Cause**: 
- Concurrency levels 24 and 32 with 3000-token inputs overwhelmed the server
- At concurrency 24: 132 requests all returned HTTP 503 (Service Unavailable)
- genai-bench failed with "No values found for metric 'ttft'"
- Server configuration `max-running-requests: 256` couldn't handle the load

**Actions Taken**:

1. **Cancelled Task 26**: Stopped failing task
2. **Cleaned up containers**: Removed leftover autotuner containers
3. **Created Task 27** with improved configuration:
   - **Reduced concurrency**: `[1, 4, 8, 16]` (removed 24, 32)
   - **Increased max_time_per_run**: `30` seconds (was 10)
   - **Kept timeout**: `1800` seconds per experiment
   - Task name: `GLM-4.6-FP8-v3`

**Configuration Changes**:
```json
"num_concurrency": [1, 4, 8, 16],     // Removed 24, 32
"max_time_per_run": 30,                // Increased from 10
"timeout_per_iteration": 1800          // 30 minutes
```

**Current Status**:
- Task 27 created and started
- Experiment 1 deploying
- Multiple containers still running (cleanup needed)

**Next Steps**:
- Clean up all containers properly
- Monitor Task 27 to verify no 503 errors occur

</details>

---

> Stop all running containers, and restart ARQ worker.

<details>
<summary>Cleaned up all containers and restarted ARQ worker</summary>

**Actions Completed**:

1. **Stopped all autotuner containers** âœ…
   - Found containers: `autotuner-GLM-4.6-FP8-v3-exp1`
   - Stopped and removed all autotuner containers
   - Including old leftover containers from previous tasks

2. **Verified GPU cleanup** âœ…
   - All 8 GPUs: 0% utilization, 97GB free
   - No processes holding GPU resources

3. **Restarted ARQ worker** âœ…
   - Killed all existing worker processes (there were duplicates)
   - Started single clean worker process
   - Worker PID: 334609

**System Status After Cleanup**:
```
Containers:   0 autotuner containers running
GPUs:         All 8 GPUs free (0% util, 97GB free each)
ARQ Worker:   1 worker process running
```

**Result**: System is clean and ready for new tasks. All resources released, single worker running with latest code (1800s timeout, cleanup on timeout).

</details>

---

> Clean up and restart task 27

<details>
<summary>Task 27 cleaned up and Task 28 started fresh</summary>

**Actions Completed**:

1. **Cancelled Task 27** âœ…
   - Status changed to CANCELLED
   - Freed up task queue

2. **Created Task 28 (GLM-4.6-FP8-v4)** âœ…
   - Same optimized configuration as Task 27
   - **Timeout**: 1800 seconds (30 minutes) per experiment
   - **Concurrency**: [1, 4, 8, 16] (no 24/32 to avoid 503 errors)
   - **Max time per run**: 30 seconds
   - Clean start with no leftover state

3. **Started Task 28** âœ…
   - Started at: 21:17:58
   - Experiment 1 deploying
   - Container: `autotuner-GLM-4.6-FP8-v4-exp1`
   - GPUs allocated: All 8 H20 GPUs (97GB each)
   - Status: Waiting for model to load (~3-5 min)

**Configuration Summary**:
```json
{
  "timeout_per_iteration": 1800,
  "num_concurrency": [1, 4, 8, 16],
  "max_time_per_run": 30,
  "traffic_scenarios": ["D(3000,300)"],
  "attention_backend": ["flashinfer", "fa3"],
  "page_size": [32, 64, 128, 256, 512]
}
```

**Expected Results**:
- 2 attention backends Ã— 5 page sizes = 10 experiments
- Each experiment with 4 concurrency levels (1, 4, 8, 16)
- No 503 errors expected with reduced concurrency
- Proper timeout and cleanup if needed

**System Status**:
- Task: GLM-4.6-FP8-v4 (ID: 28) - RUNNING
- Container: Up 15 seconds
- GPUs: All 8 allocated and loading model
- Worker: Single clean process with latest code

</details>


> Investigate and fix OME model-agent shape mapping issue

<details>
<summary>OME Model-Agent Troubleshooting - Shape label configuration</summary>

* *User*: Check why the InferenceService not ready for 300+s.

* *Claude*:
  **Investigation**:
  Task 9 showed InferenceService waiting for ready status beyond 540 seconds. Checked the deployed YAML and found empty parameter values, leading to discovery of template bug (fixed above).

* *User*: Check issues of the model-agent, run a simple test for it.

* *Claude*:
  **Model-Agent Issue Analysis**:
  
  **Error Observed**:
  ```
  FATAL: Failed to initialize components: failed to create scout: 
  couldn't find shape  in the shape mapping
  ```
  
  **Investigation Steps**:
  1. Checked OME pods: `ome-model-agent-daemonset` in CrashLoopBackOff on all GPU nodes
  2. Examined logs: All pods fail with identical shape mapping error
  3. Checked GPU hardware: 4 nodes with 8x NVIDIA H20 GPUs each (97GB memory)
  4. Verified node labels: **No nodes had `ome.io/shape` label**
  
  **Test Attempts**:
  ```bash
  # Test 1: Add H20-based shape label
  kubectl label node host-10-97-65-155 ome.io/shape=h20.8 --overwrite
  kubectl delete pod -n ome ome-model-agent-daemonset-7b5fc
  # Result: Still crashes with same error
  
  # Test 2: Try standard cloud GPU shape name
  kubectl label node host-10-97-65-155 ome.io/shape=VM.GPU.A10.1 --overwrite
  kubectl delete pod -n ome ome-model-agent-daemonset-rcjkw
  # Result: Still crashes with same error
  ```
  
  **Root Cause**:
  The error message shows empty shape value ("couldn't find shape  in the shape mapping"), suggesting:
  1. Model-agent not reading node labels properly, OR
  2. Timing issue where label isn't available at pod start, OR
  3. Hardcoded shape mapping doesn't include any valid values for this environment
  
  **Configuration Examined**:
  - ConfigMaps: No shape mapping configuration found
  - Model-agent DaemonSet: Only mounts `/mnt/data/models`, no config volumes
  - InferenceService config: Contains various settings but no shape definitions
  - Environment: Model-agent uses `NODE_NAME` env var for node identification
  
  **Impact**:
  - InferenceService `template-fix-verified-exp1` created successfully âœ…
  - But pods cannot be scheduled due to model-agent not running âŒ
  - This blocks all OME InferenceService deployments
  
  **Conclusion**:
  - Template fix verified: InferenceService deploys with correct parameters âœ…
  - OME infrastructure issue: Model-agent requires proper shape configuration âŒ
  - Next step: Examine OME source code to understand shape mapping implementation

* *User*: Learn source code from /root/work/inference-autotuner/third_party/ome, modify ome code and build it if necessary.

* *Claude*:
  **Status**: Beginning investigation of OME source code to understand and fix shape mapping issue.
  
  **Source Location**: `/root/work/inference-autotuner/third_party/ome/`
  - Model-agent main: `cmd/model-agent/main.go`
  - Need to find shape mapping logic and fix or bypass the requirement

</details>


---

## OME Fork and Model-Agent Bare-Metal Fix

> Switch OME submodule URL to https://github.com/k-l-lambda/ome.git, then build it from source and replace the installed version.

<details>
<summary>Claude: Forked OME, created bare-metal fix for model-agent, rebased to main</summary>

### User Request
Switch the OME submodule to use the forked repository at https://github.com/k-l-lambda/ome.git, then build the model-agent from source with fixes for bare-metal GPU nodes.

### Actions Taken

#### 1. Switched OME Submodule
- Updated `.gitmodules` to point to `https://github.com/k-l-lambda/ome.git`
- Ran `git submodule sync` to update configuration
- Updated remote URL in the ome submodule directory
- Successfully switched to forked repository

#### 2. Created Bare-Metal Fix

**Problem Identified:**
The OME model-agent DaemonSet was crashing on bare-metal GPU nodes with error:
```
FATAL: Failed to initialize components: failed to create scout: couldn't find shape  in the shape mapping
```

**Root Cause:**
- Model-agent expects `node.kubernetes.io/instance-type` label (cloud provider specific)
- Bare-metal nodes don't have this label
- When label is missing, scout initialization fails with empty instanceType

**Solution Implemented:**

Modified two files in `third_party/ome/`:

1. **pkg/modelagent/scout.go** - Added fallback logic:
```go
// Try the newer label first, then fallback to the deprecated beta label
instanceType, ok := nodeInfo.Labels[constants.NodeInstanceShapeLabel]
if !ok {
    instanceType = nodeInfo.Labels[constants.DeprecatedNodeInstanceShapeLabel]
}

// If no instance-type label exists (e.g., bare-metal nodes), use a default shape
if instanceType == "" {
    logger.Info("No instance-type label found on node, defaulting to 'bare-metal'")
    instanceType = "bare-metal"
}

nodeShapeAlias, err := utils.GetInstanceTypeShortName(instanceType)
```

2. **pkg/utils/instance_type_util.go** - Handle empty string:
```go
func GetInstanceTypeShortName(currentInstanceType string) (string, error) {
    // Handle empty instance type (bare-metal or missing label)
    if currentInstanceType == "" {
        return "bare-metal", nil
    }

    if shortName, ok := instanceTypeMap[currentInstanceType]; ok {
        return shortName, nil
    }
    // Return the original instance type as a fallback for unknown shapes
    return currentInstanceType, nil
}
```

#### 3. Committed and Pushed Changes
- Created branch `bare-metal-model-agent-fix`
- Committed with message: "Fix model-agent for bare-metal nodes without instance-type labels"
- Pushed to: https://github.com/k-l-lambda/ome/tree/bare-metal-model-agent-fix

#### 4. Rebased to Main Branch
- Fetched latest main branch
- Rebased `bare-metal-model-agent-fix` onto `origin/main`
- Force pushed rebased branch
- Current commit: `39f1edf`

#### 5. Build Attempts and Network Issues

**Network Connectivity Problems:**
- Direct Docker Hub access times out
- Proxy at `172.17.0.1:1081` was inaccessible (listening only on `127.0.0.1:1081`)
- Corrected proxy address to `127.0.0.1:1081` which works with curl
- buildkitd configuration challenges prevented successful image build

**Test Results:**
```bash
# Direct access - FAILS
curl -v -m 10 https://registry-1.docker.io/v2/
# Result: Connection timeout

# Proxy access (corrected) - WORKS
curl -v -m 10 -x http://127.0.0.1:1081 https://registry-1.docker.io/v2/
# Result: 200 Connection established

# buildkitd with proxy - Still fails
# Proxy configuration not properly applied to buildkit daemon
```

### Current Status

**Completed:**
- âœ… OME submodule switched to forked repository
- âœ… Bare-metal fix created and tested locally
- âœ… Changes committed to branch `bare-metal-model-agent-fix`
- âœ… Branch rebased onto main
- âœ… Changes pushed to GitHub

**Pending:**
- â³ Build Docker image (blocked by network/proxy issues)
- â³ Load image into cluster
- â³ Update DaemonSet to use new image
- â³ Verify model-agent pods start successfully

### Next Steps

**Option B (Recommended):** Build via GitHub Actions or CI/CD
- Set up GitHub Actions workflow to build model-agent image
- Use GitHub's infrastructure with better network access
- Push built image to accessible container registry
- Deploy to cluster from that registry

### Files Modified

1. `third_party/ome/pkg/modelagent/scout.go` - Scout initialization with bare-metal fallback
2. `third_party/ome/pkg/utils/instance_type_util.go` - Instance type mapping with empty string handling
3. `.gitmodules` - Updated OME submodule URL

### Technical Details

**Branch:** `bare-metal-model-agent-fix`
**Repository:** https://github.com/k-l-lambda/ome
**Commit:** 39f1edf
**Based on:** main branch (ea13090)

</details>


---

> Build model-agent Docker image with bare-metal fix via GitHub Actions

<details>
<summary>GitHub Actions workflow setup and build trigger</summary>

### Problem

After fixing the model-agent code for bare-metal node support, we needed to build the Docker image. Local build attempts failed due to:
- Network connectivity issues accessing Docker Hub
- Proxy configuration challenges with buildkitd
- Complex multi-stage Dockerfile requiring Rust and Go toolchains

### Solution: GitHub Actions Build

**Strategy:** Use GitHub's infrastructure for building since it has reliable network access and proper tooling.

#### 1. Created Custom Workflow

Created `.github/workflows/build-feature-branch.yaml` with the following features:
- **Manual trigger:** `workflow_dispatch` with branch parameter
- **Builds model-agent** from specified branch
- **Publishes to GHCR:** `ghcr.io/k-l-lambda/model-agent:<tag>`
- **Platform:** `linux/amd64` only (faster than multi-arch)
- **Uses caching:** GitHub Actions cache for faster rebuilds

**Key Configuration:**
```yaml
on:
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to build'
        required: true
        type: string

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: k-l-lambda/model-agent
  GO_VERSION: '1.24.1'
```

**Build Steps:**
1. Checkout specified branch
2. Setup Go 1.24.1
3. Setup Docker Buildx
4. Login to GitHub Container Registry (using `GITHUB_TOKEN`)
5. Build and push image with tag: `<branch-name>-<short-sha>`

#### 2. Workflow Deployment Process

**Initial Attempt:** Created workflow on feature branch - didn't work because GitHub only recognizes workflows from the default branch (main).

**Solution:**
```bash
# Switch to main branch
git checkout main
git pull origin main

# Cherry-pick workflow commit from feature branch
git cherry-pick a4d50f1

# Push to main
git push origin main
```

**Result:** Workflow now available at `https://github.com/k-l-lambda/ome/actions/workflows/build-feature-branch.yaml`

#### 3. Triggering the Build

Used GitHub API to trigger workflow:
```bash
GITHUB_TOKEN="<token>"
curl -X POST \
  -H "Accept: application/vnd.github+json" \
  -H "Authorization: Bearer $GITHUB_TOKEN" \
  -H "X-GitHub-Api-Version: 2022-11-28" \
  "https://api.github.com/repos/k-l-lambda/ome/actions/workflows/build-feature-branch.yaml/dispatches" \
  -d '{"ref":"main","inputs":{"branch":"bare-metal-model-agent-fix"}}'
```

**Response:** HTTP 204 (success) - workflow triggered successfully

#### 4. Expected Output

**Image Location:**
```
ghcr.io/k-l-lambda/model-agent:bare-metal-model-agent-fix-<short-sha>
```

**Build Time:** Approximately 5-10 minutes

**Next Steps After Build Completes:**
1. Pull image to cluster: `nerdctl pull ghcr.io/k-l-lambda/model-agent:<tag>`
2. Find model-agent DaemonSet: `kubectl get daemonset -A | grep model-agent`
3. Update DaemonSet image specification
4. Restart model-agent pods
5. Verify InferenceService deployment succeeds

### Technical Notes

**Why This Approach:**
- âœ… Avoids local network connectivity issues
- âœ… GitHub provides reliable infrastructure
- âœ… Uses official GitHub Actions for Docker builds
- âœ… Automatic authentication with GITHUB_TOKEN
- âœ… Build caching speeds up subsequent builds

**Workflow Features:**
- Single platform build (`linux/amd64`) for speed
- Uses Docker Buildx for modern build features
- Leverages GitHub Actions cache for Go modules and Docker layers
- Generates build summary with pull command

### Files Created

1. `.github/workflows/build-feature-branch.yaml` - Custom workflow for feature branch builds

### Commits

- **Feature branch:** `a4d50f1` - Added workflow to bare-metal-model-agent-fix branch
- **Main branch:** `bcbe55b` - Cherry-picked workflow to main for GitHub recognition

### Repository State

**Branch:** `bare-metal-model-agent-fix` (commit: `39f1edf` + workflow commit)
**Repository:** https://github.com/k-l-lambda/ome
**Workflow:** https://github.com/k-l-lambda/ome/actions/workflows/build-feature-branch.yaml

</details>

---

## 2025-11-18: OME ClusterServingRuntime Args Fix & Model Storage Investigation

> Re-run an ome task to test it.
> 
> Investigate ClusterServingRuntime, utilize the source code of ome. Build it if necessary.
> 
> Investigate how to store model weights by reading documents in ome project, there should be a common service for storage.

<details>
<summary>Fixed ClusterServingRuntime args configuration and investigated OME model storage architecture</summary>

### Problem: ClusterServingRuntime Args Configuration

The InferenceService pods were crashing with two issues:
1. Invalid SGLang arguments (`--enable-metrics`, `--log-requests`)
2. Args formatted with pipe characters (YAML literal block scalars)

**Root Cause Investigation:**

Examined the OME source code `/root/work/inference-autotuner/third_party/ome/pkg/controller/v1beta1/inferenceservice/utils/merging.go` and found the args merging behavior. The original ClusterServingRuntime presets used:
```python
"command": ["/bin/bash", "-lc", "--"],
"args": [
    "python3 -m sglang.launch_server \\\n"
    "--host=0.0.0.0 \\\n"
    "--enable-metrics \\\n"  # Invalid argument!
    "--log-requests \\\n"    # Invalid argument!
    ...
]
```

**Solution Applied:**

1. **Removed invalid arguments**: `--enable-metrics` and `--log-requests` are not valid SGLang arguments
2. **Changed command/args format**: Switched from bash wrapper to direct Python command execution

Updated all ClusterServingRuntime presets in `/root/work/inference-autotuner/src/config/clusterservingruntime_presets.py`:
- SGLang small/large variants
- vLLM small/large variants  
- Mixtral MoE variant

**After fix:**
```python
"command": ["python3", "-m", "sglang.launch_server"],
"args": [
    "--host=0.0.0.0",
    "--port=8080",
    "--model-path=$(MODEL_PATH)",
    "--tp-size=1",
    "--mem-frac=0.9"
],
```

**Result:** Pod args now correctly formatted without pipe characters or invalid flags.

### OME Model Storage Architecture Investigation

Read OME documentation and source code to understand model weight storage:

**Storage Backends Supported:**
1. **OCI Object Storage**: Cloud storage (via ome-agent)
2. **HuggingFace**: Direct HF Hub downloads (via ome-agent)
3. **S3**: S3-compatible storage
4. **PVC**: Kubernetes Persistent Volume Claims (OEP-0004 feature)
5. **Host Path**: Local node storage at `/mnt/data/models`

**Current Architecture (Host Path Storage):**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Model Agent DaemonSet (runs on each node)                   â”‚
â”‚ - Downloads models from OCI/HF/S3 to /mnt/data/models      â”‚
â”‚ - Volume: hostPath /mnt/data/models                         â”‚
â”‚ - Labels nodes where models are available                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ClusterBaseModel Resource                                    â”‚
â”‚ spec:                                                         â”‚
â”‚   storage:                                                    â”‚
â”‚     storageUri: "hf://meta-llama/Llama-3.2-1B-Instruct"     â”‚
â”‚     path: "/raid/models/meta/llama-3-2-1b-instruct"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â”‚
                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ InferenceService Pod                                         â”‚
â”‚ - Mounts hostPath volume using storage.path                 â”‚
â”‚ - Code: UpdatePodSpecVolumes() in base.go:                  â”‚
â”‚   podSpec.Volumes = append(podSpec.Volumes, Volume{         â”‚
â”‚     Name: baseModelName,                                     â”‚
â”‚     VolumeSource: corev1.VolumeSource{                       â”‚
â”‚       HostPath: &corev1.HostPathVolumeSource{               â”‚
â”‚         Path: *baseModel.Storage.Path  // /raid/models/...  â”‚
â”‚       }                                                       â”‚
â”‚     }                                                         â”‚
â”‚   })                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Code Location:**
`/root/work/inference-autotuner/third_party/ome/pkg/controller/v1beta1/inferenceservice/components/base.go:262`
```go
func UpdatePodSpecVolumes(...) {
    if b.BaseModel != nil && b.BaseModel.Storage != nil && b.BaseModel.Storage.Path != nil {
        modelVolume := corev1.Volume{
            Name: b.BaseModelMeta.Name,
            VolumeSource: corev1.VolumeSource{
                HostPath: &corev1.HostPathVolumeSource{
                    Path: *b.BaseModel.Storage.Path,  // Uses storage.path directly!
                },
            },
        }
        podSpec.Volumes = append(podSpec.Volumes, modelVolume)
    }
}
```

**Current Issue Identified:**

The pod is crashing with:
```
model_path='/raid/models/meta/llama-3-2-1b-instruct'
TypeError: 'NoneType' object is not subscriptable
```

**Root Cause:** Path mismatch between ClusterBaseModel and actual node storage:
- **ClusterBaseModel**: `storage.path: /raid/models/meta/llama-3-2-1b-instruct` 
- **Model Agent DaemonSet**: Mounts `/mnt/data/models` (not `/raid/models`)
- **InferenceService Pod**: Tries to mount hostPath `/raid/models/...` which doesn't exist!

The model files exist at `/mnt/data/models/meta/llama-3-2-1b-instruct` on nodes, but the ClusterBaseModel spec points to `/raid/models/...`.

**PVC Storage Support (OEP-0004):**

Alternative approach for shared storage using PVCs:
```yaml
# ClusterBaseModel with PVC
spec:
  storage:
    storageUri: "pvc://namespace:pvc-name/sub-path"
```

Benefits:
- No host path required
- Supports RWX (NFS) for multi-pod access
- Supports RWO (block storage) for single-replica
- Model agent skips PVC types
- InferenceService directly mounts PVC

**Documentation Read:**
- `/root/work/inference-autotuner/third_party/ome/oeps/0004-pvc-storage-support/README.md`
- `/root/work/inference-autotuner/third_party/ome/cmd/ome-agent/README.md`

**Next Steps:**
1. Fix the storage.path mismatch issue in ClusterBaseModel
2. Either update ClusterBaseModel to use `/mnt/data/models` path
3. Or investigate why the path is set to `/raid/models` instead of `/mnt/data/models`

</details>


---

## OME PVC Support - Docker Image Build and Deployment

> Check build status and continue deployment of OME controller with PVC support.

<details>
<summary>Successfully built and deployed OME controller after resolving Docker image compatibility issues (6 iterations)</summary>

**User**: Check build status and continue.

**Claude**: 
After a comprehensive debugging journey through 6 different Docker base image configurations, successfully built and deployed the OME controller with PVC support:

### The Journey (6 Iterations):

1. âŒ **`gcr.io/distroless/static:nonroot`** - Error: `exec /manager: no such file or directory`
   - Issue: No CGO support for the XET Rust library

2. âŒ **`gcr.io/distroless/base-debian11:nonroot`** - Error: `libssl.so.3: cannot open shared object file`
   - Issue: Missing OpenSSL libraries required by XET

3. âŒ **`gcr.io/distroless/cc-debian11:nonroot`** - Error: Same libssl.so.3 error
   - Issue: Debian 11 has OpenSSL 1.1, binary requires OpenSSL 3.x from golang:1.24 builder

4. âŒ **`gcr.io/distroless/cc-debian12:nonroot`** - Error: `GLIBC_2.38' not found`
   - Issue: golang:1.24 has glibc 2.38, distroless image has older version
   - Progress: New error means OpenSSL issue fixed!

5. âŒ **`debian:bookworm-slim`** (3 attempts) - Build failures
   - Tried: `useradd`, `adduser` commands
   - Issue: User creation failing in minimal Debian image during build

6. âœ… **`golang:1.24`** (same as builder) - **SUCCESS!**
   - Solution: Use identical image for both builder and runtime
   - Result: Perfect compatibility for glibc 2.38, OpenSSL 3.x, all dependencies
   - Trade-off: Larger image (~1GB vs ~100MB distroless) but guaranteed to work

### Key Technical Discoveries:

- **CGO Requirements**: Manager binary uses CGO to link against XET Rust library
- **OpenSSL Version Matching**: golang:1.24 (Debian 12) requires OpenSSL 3.x
- **glibc Version Matching**: Runtime must match builder's glibc 2.38
- **Image Caching**: Set `imagePullPolicy: Always` and `no-cache: true` to avoid stale images

### Final Build & Deployment:

**Commit**: d06bca1 (main repo), 797f52b (OME submodule)
**Build Time**: 4.5 minutes (17:06-17:10 CST)
**Image**: `ghcr.io/novitalabs/ome-manager:pvc-support`

**Deployment Status**: âœ…
```
ome-controller-manager-6db8bf8868-6xj4w   1/1   Running
ome-controller-manager-6db8bf8868-chr7w   1/1   Running  
ome-controller-manager-6db8bf8868-tm7gk   1/1   Running
```

**Logs**: Normal controller startup, no library errors âœ…

### PVC Implementation (Preserved):

The PVC support code from previous session remained intact throughout:
- Annotation-based configuration (`ome.io/storage-type: "pvc"`)
- PersistentVolumeClaim volume creation with subPath support
- Node selector skip for PVC storage (not needed unlike hostPath)
- Backward compatibility with existing hostPath storage

### Documentation Created:

- `/tmp/FINAL_STATUS.md` - Complete journey and status report
- All intermediate debugging docs preserved

**Status**: Ready for PVC functionality testing ğŸš€

</details>


---

