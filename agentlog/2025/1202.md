
## SLO Batch-Level Filtering Bug Investigation and Fix

> Check task 12 to see if the SLO constraint is correctly applied to each batch of each experiment, and whether the final results correctly pass the SLO filter. Pay particular attention to the maximum throughput value.

<details>
<summary>Investigation and Resolution</summary>

### Problem Discovery

Task 12 reported `max_throughput = 264.48 tokens/s` from a non-SLO-compliant batch (concurrency_12, P90=7.54s exceeds threshold 6.5s), instead of the correct value `231.70 tokens/s` from the only SLO-compliant batch (concurrency_8, P90=5.71s).

This indicated that batch-level SLO filtering was not working despite the code being deployed.

### Investigation Timeline

1. **Verified Deployment Timeline**
   - SLO filtering code committed: 2025-12-01 10:52
   - Worker started: 2025-12-02 17:29
   - Task 12 executed: 2025-12-02 17:46-19:00
   - Conclusion: Worker **should** have had the new code

2. **Added Strategic DEBUG Logs**

   Modified `src/orchestrator.py` line 275:
   ```python
   if "slo" in task:
       benchmark_config_with_slo["slo_config"] = task["slo"]
       slo_value = benchmark_config_with_slo.get("slo_config")
       print(f"[DEBUG ORCHESTRATOR] slo_config value: {slo_value}")
   ```

   Modified `src/controllers/direct_benchmark_controller.py` line 584-585:
   ```python
   print(f"[DEBUG] _parse_results: slo_config type={type(slo_config)}, value={slo_config}")
   print(f"[DEBUG] _parse_results: all_metrics count={len(all_metrics) if all_metrics else 0}")
   ```

3. **Created Test Task 18** (Debug-SLO-Test with Qwen2.5-1.5B-Instruct)

   **Critical Finding from Logs:**
   ```
   [DEBUG] _parse_results: slo_config type=<class 'NoneType'>, value=None
   ```

   The slo_config was **None** at the benchmark controller! But orchestrator's DEBUG log never appeared, suggesting the bug was upstream.

### Root Cause Identified

Found the bug in `src/web/workers/autotuner_worker.py` line 322-334:

**BEFORE (Buggy Code):**
```python
task_config = {
    "task_name": task.task_name,
    "description": task.description or "",
    "model": task.model_config,
    "base_runtime": task.base_runtime,
    "runtime_image_tag": task.runtime_image_tag,
    "parameters": task.parameters,
    "optimization": task.optimization_config,
    "benchmark": task.benchmark_config,
    "deployment_mode": task.deployment_mode,
    "clusterbasemodel_config": task.clusterbasemodel_config,
    "clusterservingruntime_config": task.clusterservingruntime_config,
    # ❌ Missing: "slo": task.slo_config
}
```

The worker constructs task_config from the database Task object but **omitted the `"slo"` field entirely**. This caused:
- Orchestrator's `if "slo" in task:` check to fail
- slo_config never merged into benchmark_config
- Benchmark controller received `slo_config=None`
- **SLO filtering code never executed**

### Fix Applied

**AFTER (Fixed Code):**
```python
task_config = {
    "task_name": task.task_name,
    "description": task.description or "",
    "model": task.model_config,
    "base_runtime": task.base_runtime,
    "runtime_image_tag": task.runtime_image_tag,
    "parameters": task.parameters,
    "optimization": task.optimization_config,
    "benchmark": task.benchmark_config,
    "deployment_mode": task.deployment_mode,
    "clusterbasemodel_config": task.clusterbasemodel_config,
    "clusterservingruntime_config": task.clusterservingruntime_config,
    "slo": task.slo_config,  # ✅ Added this line
}
```

### Verification with Task 19

Created Task 19 (SLO-Filter-FIXED) with strict SLO thresholds (P90 ≤ 1.5s).

**Success - Complete Log Output:**
```
[DEBUG ORCHESTRATOR] slo_config value: {'latency': {'p90': {'threshold': 1.5, 'weight': 2, 'hard_fail': True, 'fail_ratio': 0.2}}, 'steepness': 0.1}
[DEBUG] _parse_results: slo_config type=<class 'dict'>, value={'latency': {'p90': {'threshold': 1.5, ...}}}
[DEBUG] _parse_results: all_metrics count=3
[Benchmark] Filtering 3 batches by SLO compliance...
[Benchmark] ✓ 3/3 batches passed SLO
[Optimizer] Score: -1232.1446 (no SLO violations)
```

**Confirmation:**
- ✅ slo_config correctly passed from worker → orchestrator → benchmark controller
- ✅ Batch-level filtering executed: "Filtering 3 batches by SLO compliance..."
- ✅ All batches evaluated: "✓ 3/3 batches passed SLO"
- ✅ Objective score calculated without violations

### Files Modified

1. **src/web/workers/autotuner_worker.py** (line 334) - Added `"slo": task.slo_config` to task_config dictionary
2. **src/orchestrator.py** (line 275) - Added DEBUG log for slo_config value
3. **src/controllers/direct_benchmark_controller.py** (lines 584-585) - Added DEBUG logs for slo_config type/value

### Lessons Learned

1. **Data Flow Tracing**: Strategic DEBUG logging at each pipeline stage is essential for debugging data flow issues
2. **Small Model Testing**: Using 1.5B model (Qwen2.5-1.5B-Instruct) instead of 12B enabled rapid testing without OOM issues
3. **Configuration Propagation**: Worker's task_config construction is the critical entry point - any omission breaks downstream functionality
4. **Worker Restart**: Must restart ARQ worker after code changes (not just web server)

</details>

---
