

## Session: OME Controller Troubleshooting & End-to-End Testing (2025-11-20)

> Task 12 is still failed, keep fix it until it can run successfully.

<details>
<summary>Fixed OME controller crash, resolved multiple port-forward/benchmark issues, achieved first successful end-to-end OME task</summary>

### Problem: Task 12 Failed - Model Not Downloading Automatically

**User Question**: "Why its model not download automatically as expected?"

#### Investigation Results:

**Task 12 Configuration**:
- Model: `meta-llama/Llama-3.2-3B-Instruct`
- Issue: InferenceService timing out after 600s waiting to be ready

**Root Causes Discovered**:

1. **Missing ClusterBaseModel**: No ClusterBaseModel for "llama-3-2-3b-instruct" existed
2. **Model Name Sanitization Missing**: Code wasn't sanitizing model names from task config before creating resources
3. **Storage Path Mismatch** (Critical): Model-agent DaemonSet mounts `/mnt/data/models` from host, but ClusterBaseModel specified `/raid/models`, causing model to download to container-local path inaccessible to InferenceService pods

#### Fixes Applied (Task 12):

**File**: `src/controllers/ome_controller.py` (lines 143-145)
```python
# Sanitize model_name to match ClusterBaseModel naming convention
# Convert "meta-llama/Llama-3.2-3B-Instruct" to "llama-3-2-3b-instruct"
safe_model_name = sanitize_dns_name(model_name)
```

**ClusterBaseModel Created**: `/tmp/llama-3-2-3b-clusterbasemodel-fixed.yaml`
- Corrected storage path from `/raid/models/...` to `/mnt/data/models/meta/llama-3-2-3b-instruct`
- Model re-downloaded to accessible location
- Verified: InferenceService pods started successfully with "The server is fired up and ready to roll!"

---

### Critical Issue: OME Controller Manager Crash

**Symptoms**:
- All 3 OME controller manager pods in CrashLoopBackOff
- Webhook server unavailable (connection refused)
- InferenceService creation failing

**Error in Logs**:
```
Failed to start manager: failed to wait for acceleratorclass caches to sync:
timed out waiting for cache to be synced for Kind *v1beta1.AcceleratorClass
```

#### Root Cause Analysis:

**Missing CRD**: AcceleratorClass CustomResourceDefinition not installed
- Controller image: `ghcr.io/novitalabs/ome-manager:pvc-support` (custom build)
- Image contained code referencing AcceleratorClass API
- Corresponding CRD was never applied during installation

**Impact**:
- Controllers couldn't initialize cache watchers
- Start timeout after waiting for cache sync
- Entire OME system non-functional

#### Solution:

**Found CRD**: `third_party/ome/config/crd/full/ome.io_acceleratorclasses.yaml`

**Applied Fix**:
```bash
kubectl apply -f /root/work/inference-autotuner/third_party/ome/config/crd/full/ome.io_acceleratorclasses.yaml
kubectl delete pod ome-controller-manager-6db8bf8868-8wvlb ome-controller-manager-6db8bf8868-9bhnx -n ome
```

**Results**:
- ✅ All 3 controller manager pods: Running and healthy
- ✅ Webhook server: Responding on port 9443
- ✅ InferenceServices: Creation working

**Documentation**: `/tmp/OME_CONTROLLER_FIX_REPORT.md`

---

### Issue: Task 17-18 Failed - Port-Forward Problems

**Tasks Created for Testing**:
- Task 13: Failed (Docker mode, wrong deployment_mode)
- Task 14: Stuck (webhook down, task started before fix)
- Task 15: Failed (webhook connection refused - before controller fix)
- Task 16-17: Failed (port-forward issues)
- Task 18: Failed (used old code before worker restart)

#### Port-Forward Issues Discovered:

**File**: `src/controllers/direct_benchmark_controller.py`

**Issue 1 - Wrong Label Selector** (line 138):
```python
# WRONG: Using KServe label
f"serving.kserve.io/inferenceservice={service_name}"

# CORRECT: Using OME label  
f"ome.io/inferenceservice={service_name}"
```

**Issue 2 - Missing Service Name Suffix** (lines 151, 159):
```python
# WRONG: Service name without suffix
pod_or_svc = f"svc/{service_name}"

# CORRECT: OME services have -engine suffix
pod_or_svc = f"svc/{service_name}-engine"
```

**Issue 3 - Wrong Port** (line 110, 245):
```python
# WRONG: Default remote_port=8000
def setup_port_forward(self, service_name: str, namespace: str, remote_port: int = 8000, ...)
endpoint_url = self.setup_port_forward(service_name, namespace, 8000, local_port)

# CORRECT: OME services run on 8080
def setup_port_forward(self, service_name: str, namespace: str, remote_port: int = 8080, ...)
endpoint_url = self.setup_port_forward(service_name, namespace, 8080, local_port)
```

**Verification**: Task 19 logs showed port-forward succeeded:
```
[Port Forward] Found pod: ome-fixed-test-exp1-engine-5ff78c6954-6r6cg
[Port Forward] Established: http://localhost:8080
```

---

### Issue: Benchmark Failed - Proxy & Token Problems

**User Request**: "Try to download the tokenizer by huggingface hub cli with this proxy."

#### HuggingFace Download Failures:

**Error in Task 19 Logs**:
```
Connection to 172.17.0.1 timed out. (connect timeout=10)
Unable to connect to proxy
```

#### Investigation & Fixes:

**Test 1 - Proxy Connectivity**:
```bash
# Docker bridge IP (172.17.0.1:1081) - TIMEOUT ❌
# Localhost (127.0.0.1:1081) - WORKS ✅
```

**Test 2 - Authentication**:
```bash
export HTTPS_PROXY=http://127.0.0.1:1081
huggingface-cli login --token hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Successfully downloaded tokenizer ✅
```

**Fix 1 - Proxy URL** (direct_benchmark_controller.py:311):
```python
# WRONG: Docker bridge IP
proxy_url = os.environ.get('HTTPS_PROXY') or 'http://172.17.0.1:1081'

# CORRECT: Localhost
proxy_url = os.environ.get('HTTPS_PROXY') or 'http://127.0.0.1:1081'
```

**Fix 2 - HuggingFace Token** (scripts/start_worker.sh:19):
```bash
# Added HF_TOKEN environment variable
export HF_TOKEN="hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```

---

### Success: Task 23 - First Complete End-to-End Test

**Task Configuration**: OME-Final-Working-Test
- Model: meta-llama/Llama-3.2-1B-Instruct
- Runtime: sglang-llama-small
- Deployment: OME mode
- Parameters: tp-size=[1]
- Benchmark: text-to-text with D(100,100) traffic

**Timeline**:
```
[0s]  RUNNING   | Creating InferenceService
[30s] RUNNING   | Waiting for service ready
[60s] RUNNING   | Service ready, running benchmark
[90s] COMPLETED | Experiment successful ✅
```

**Results**: 1/1 experiments successful

**Verification Steps**:
1. ✅ InferenceService created with correct ClusterBaseModel
2. ✅ Pod deployed with correct labels and service name
3. ✅ Port-forward established using ome.io label
4. ✅ genai-bench downloaded tokenizer via proxy
5. ✅ Benchmark completed successfully
6. ✅ Results collected and stored

---

### Summary of All Fixes

| Component | Issue | Fix | File |
|-----------|-------|-----|------|
| OME Controller | Missing AcceleratorClass CRD | Applied CRD YAML | N/A (kubectl apply) |
| Model Naming | Not sanitizing names | Added sanitize_dns_name() | ome_controller.py:143-145 |
| Storage Path | Wrong mount path | Updated ClusterBaseModel | ClusterBaseModel YAML |
| Port-Forward Label | Wrong label selector | Changed to ome.io/inferenceservice | direct_benchmark_controller.py:138 |
| Service Name | Missing -engine suffix | Added suffix | direct_benchmark_controller.py:151,159 |
| Port Config | Wrong port (8000) | Changed to 8080 | direct_benchmark_controller.py:110,245 |
| Proxy URL | Docker bridge IP timeout | Changed to 127.0.0.1 | direct_benchmark_controller.py:311 |
| HF Token | Not set in environment | Added to worker startup | start_worker.sh:19 |

### Documentation Created

- `/tmp/OME_CONTROLLER_FIX_REPORT.md` - Detailed controller crash analysis and fix

### Final Status

**OME System**: ✅ Fully operational
- 3/3 controller pods running
- Webhook server responding
- InferenceService creation/deployment working
- Port-forward and benchmarking functional
- End-to-end tasks completing successfully

**Key Achievement**: First successful OME mode autotuning task completed with proper:
- Model deployment via ClusterBaseModel
- Service discovery via OME labels
- Benchmark execution with proxy/token auth
- Results collection and scoring

</details>

---

## Cluster-wide GPU Monitoring in Dashboard

> Extend GPU Status panel in dashboard for ome environment, to watch all GPUs in entire cluster.
> Can we show GPU memory occupancy rate for cluster GPUs?
> We are working on .157, why the data of .157 node is not consistent with local data?

<details>
<summary>Implementation of cluster-wide GPU monitoring with real-time metrics and data consistency fixes</summary>

### User Requirements

1. Extend GPU Status panel to show all GPUs across the entire Kubernetes cluster (4 nodes × 8 GPUs = 32 total)
2. Display GPU memory occupancy rates for cluster GPUs
3. Fix data inconsistency between local and cluster views for node .157

### Implementation Details

**Backend Enhancement** (`src/web/routes/dashboard.py`):

Added `/api/dashboard/cluster-gpu-status` endpoint with:
- Cluster-wide GPU discovery via `kubectl get nodes -o json`
- Parsing `nvidia.com/gpu` capacity/allocatable from node status
- Real-time GPU metrics collection:
  - **Local node (.157)**: Direct `nvidia-smi` execution
  - **Remote nodes**: `kubectl exec` in pods with GPU resource requests
- Metrics collected: memory total/used/free, utilization %, temperature

**Frontend Enhancement** (`frontend/src/pages/Dashboard.tsx`):

Enhanced GPU Status card with:
- Toggle between "Local" and "Cluster" views (default: cluster)
- Grouped display by node name
- Color-coded visual indicators:
  - Green border: GPU allocatable (available for scheduling)
  - Red border: GPU not allocatable (already allocated)
  - Memory bars: Blue (< 70%), Yellow (70-90%), Red (> 90%)
  - Utilization badges: Green (< 50%), Yellow (50-80%), Red (> 80%)
- Hover tooltips with full metrics
- Graceful fallback: "No metrics" for GPUs without running pods

**API Service** (`frontend/src/services/dashboardApi.ts`):

Added TypeScript interface and API client function:
```typescript
export interface ClusterGPUStatus {
  available: boolean;
  mode: 'cluster';
  nodes?: Array<{
    index: number;
    node_name: string;
    name: string;
    capacity: number;
    allocatable: number;
    has_metrics?: boolean;
    memory_total_mb?: number;
    memory_used_mb?: number;
    memory_free_mb?: number;
    memory_usage_percent?: number;
    utilization_percent?: number;
    temperature_c?: number;
    is_local?: boolean;
  }>;
  total_gpus?: number;
  total_allocatable_gpus?: number;
  error?: string;
  timestamp: string;
}
```

### Data Consistency Fix

**Problem**: Local view and cluster view showed different data for node .157 (the API server host).

**Root Cause**:
- Local view used direct `nvidia-smi` (correct, shows real data)
- Cluster view was trying to exec into random pods (most don't have GPU access)
- This caused inconsistent/missing data for .157

**Solution**:
```python
# Detect local hostname
local_hostname = subprocess.run(["hostname"], capture_output=True, text=True).stdout.strip()

# For local node (.157): use direct nvidia-smi
if node_name == local_hostname:
    nvidia_result = subprocess.run(["nvidia-smi", "--query-gpu=..."], ...)
# For remote nodes: find pods with GPU access
else:
    # Find pods with nvidia.com/gpu in resource requests/limits
    if any("nvidia.com/gpu" in key for key in list(limits.keys()) + list(requests.keys())):
        target_pod = pod_name
        # Then exec nvidia-smi in that pod
```

**Verification**:
```bash
# Local view (GPU 0): 96.2% memory, 85% utilization, 46°C
# Cluster view for .157 (GPU 0): 96.2% memory, 88% utilization, 45°C
# ✅ Data now consistent (minor differences due to polling intervals)
```

### Metrics Availability Across Cluster

After implementation:
- **Node .157 (local)**: 8/8 GPUs with metrics ✅
- **Node .143**: 1/8 GPUs with metrics (has GPU-enabled pod running) ✅
- **Nodes .155, .156**: 0/8 GPUs with metrics (no GPU-enabled pods currently)

This is expected behavior - remote nodes only show metrics when GPU-enabled pods are running.

### User Experience

1. **Quick Toggle**: Switch between detailed local view and cluster overview
2. **Visual Scanning**: Color-coded indicators for quick status assessment
3. **Detailed Info**: Hover tooltips show full metrics without cluttering UI
4. **Consistent Data**: Local node always shows accurate real-time metrics
5. **Scalability**: Handles 32 GPUs across 4 nodes efficiently

### Files Modified

- `/root/work/inference-autotuner/src/web/routes/dashboard.py` (lines 218-441)
- `/root/work/inference-autotuner/frontend/src/pages/Dashboard.tsx` (lines 36, 52-58, 94-279)
- `/root/work/inference-autotuner/frontend/src/services/dashboardApi.ts` (lines 13-44)

</details>

---

## Intelligent GPU Allocation for OME Deployments

> Now when deploy an ome task, find idle GPUs in whole cluster

<function_calls>
Implemented automatic GPU node selection for OME deployments based on real-time cluster utilization

---

## Intelligent GPU Allocation for OME Deployments

> Now when deploy an ome task, find idle GPUs in whole cluster

<details>
<summary>Implemented automatic GPU node selection based on real-time cluster utilization</summary>

### Implementation

Created a comprehensive GPU allocation system that intelligently selects nodes with idle GPUs for OME deployments.

**New Files Created:**
1. `src/utils/gpu_discovery.py` - GPU discovery and node selection utility (445 lines)
2. `docs/GPU_ALLOCATION.md` - Comprehensive documentation

**Modified Files:**
1. `src/controllers/ome_controller.py` - Integrated GPU selection logic
2. `src/templates/inference_service.yaml.j2` - Added conditional node affinity

### Features

1. **Cluster-wide GPU Discovery**:
   - Queries all nodes via `kubectl get nodes -o json`
   - Collects real-time metrics (utilization, memory, temperature) using nvidia-smi
   - Handles local node (direct nvidia-smi) and remote nodes (kubectl exec)
   - Graceful handling of nodes without metrics

2. **Intelligent Selection Algorithm**:
   - Idle GPU definition: < 30% utilization AND < 50% memory usage
   - Ranking criteria:
     1. Nodes with idle GPUs (highest priority)
     2. Idle GPU count (more is better)
     3. Lowest average utilization
   - Falls back to Kubernetes scheduler if no suitable node found

3. **Automatic Node Affinity**:
   - Adds `kubernetes.io/hostname` node affinity to InferenceService
   - Only applies when a node is selected
   - Maintains backward compatibility

### Usage

**Automatic (Default)**:
```python
controller = OMEController()
controller.deploy_inference_service(
    task_name="my-task",
    experiment_id="1",
    namespace="autotuner",
    model_name="llama-3-2-1b-instruct",
    runtime_name="sglang-llama-small",
    parameters={"tp-size": 1}
    # GPU selection happens automatically
)
```

**Manual Disable**:
```python
controller.deploy_inference_service(
    ...,
    enable_gpu_selection=False  # Use Kubernetes scheduler
)
```

### Testing Results

**Cluster Status** (4 nodes, 32 GPUs total):
- Node .155: 8 idle GPUs (0% util, 0% mem) ← **Selected**
- Node .156: 8 idle GPUs (0% util, 0% mem)
- Node .143: 0 idle GPUs (0% util, 96.5% mem)
- Node .157: 0 idle GPUs (0% util, 96.2% mem)

**Selection Output**:
```
=== GPU Node Selection ===
Looking for node with 1 idle GPU(s)...
Selected node 'host-10-97-65-155' for deployment:
  - Allocatable GPUs: 8/8
  - Idle GPUs: 8
  - Avg Utilization: 0.0%
  - Avg Memory Usage: 0.0%
✓ Selected node: host-10-97-65-155
==========================
```

### Import Error Fix

**Problem**: Initial relative import caused `ImportError: attempted relative import beyond top-level package`

**Solution**: Changed from relative to absolute import with dynamic path insertion:
```python
import sys
from pathlib import Path

# Add parent directory to path if needed
if str(Path(__file__).parent.parent) not in sys.path:
    sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.gpu_discovery import find_best_node_for_deployment
```

**Required**: Cleared Python cache (`__pycache__` directories) and restarted ARQ worker

### Benefits

1. **Load Balancing**: Distributes workloads across idle nodes automatically
2. **Performance**: Experiments run faster on less-loaded GPUs
3. **Resource Efficiency**: Avoids oversubscribing busy nodes
4. **Transparency**: Clear logging shows selection reasoning
5. **Flexibility**: Can be disabled per deployment if needed

### Documentation

Created comprehensive documentation in `docs/GPU_ALLOCATION.md` covering:
- Architecture and design patterns
- Usage examples (automatic and manual)
- Selection criteria and configurable thresholds
- Troubleshooting guide
- Future enhancement ideas

</details>

---

## Bug Fix - Benchmark NoneType Error in Environment Variables

> **User**: Continue with the last task (fixing Task 24/25 failures)

<details>
<summary>Fixed critical bug causing benchmark execution to fail with NoneType error</summary>

### Problem Discovery

After fixing the InferenceService deployment issues (affinity placement, model name conversion), experiments were still failing during the benchmark phase with the error:

```
[Benchmark] Error running genai-bench: expected str, bytes or os.PathLike object, not NoneType
```

The InferenceService was successfully deployed and reached Ready state, port-forward was established, but genai-bench failed to execute.

### Root Cause Analysis

**File**: `src/controllers/direct_benchmark_controller.py:312-315`

**Bug**: Environment variables were being set to `None` when no proxy was configured:

```python
# Check if proxy is configured in environment or use default
proxy_url = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')  # Returns None if not set
env['HTTP_PROXY'] = proxy_url  # ❌ Setting to None
env['http_proxy'] = proxy_url  # ❌ Setting to None
env['HTTPS_PROXY'] = proxy_url  # ❌ Setting to None
env['https_proxy'] = proxy_url  # ❌ Setting to None
```

**Why it fails**: Python's `subprocess.run()` and `subprocess.Popen()` require environment variable values to be strings (or bytes/os.PathLike objects). When `proxy_url` is `None`, setting `env[key] = None` violates this constraint, causing the subprocess to fail with a TypeError.

### Solution

Added conditional check to only set proxy environment variables when proxy_url is not None:

```python
# Check if proxy is configured in environment or use default
proxy_url = os.environ.get('HTTPS_PROXY') or os.environ.get('https_proxy')
if proxy_url:
    # Only set proxy env vars if proxy is configured (env vars must be strings, not None)
    env['HTTP_PROXY'] = proxy_url
    env['http_proxy'] = proxy_url
    env['HTTPS_PROXY'] = proxy_url
    env['https_proxy'] = proxy_url
env['NO_PROXY'] = 'localhost,127.0.0.1,.local'
env['no_proxy'] = 'localhost,127.0.0.1,.local'
```

### Verification

Ran test with `/tmp/task_final.json` (Llama-3.2-3B model):

**Before fix**:
```
[Benchmark] Using proxy: None
[Benchmark] Error running genai-bench: expected str, bytes or os.PathLike object, not NoneType
```

**After fix**:
```
[Benchmark] Using proxy: None
[Benchmark] Completed in 171.8s
[Benchmark] Exit code: 1
[genai-bench] INFO Welcome to genai-bench 0.0.2!
...
```

genai-bench executed successfully for 171.8 seconds. Exit code 1 was due to HuggingFace connectivity timeout (needs proxy for international network access), not the NoneType error.

### Impact

This bug affected:
- **Direct benchmark mode** (used by both Docker and OME deployment modes with `--direct` flag)
- **All environments** where `HTTPS_PROXY` or `https_proxy` environment variables were not set
- Prevented any benchmarks from running, making experiments fail at Step 3/4

### Related Issues

After fixing this bug, discovered that genai-bench needs network access to download tokenizers from HuggingFace. According to CLAUDE.local.md, a proxy is available at `http://127.0.0.1:1081`. The fix ensures proxy settings are properly passed to genai-bench when they are configured.

Next steps: Configure proxy environment variables and rerun the task.

</details>

---

## Enhanced Tokenizer Pre-Download with Automatic Caching

**User Request**: "Looking at your current solution, will it still fail if the tokenizer is not cached?"

<details>
<summary>Implemented automatic tokenizer pre-download with proxy support</summary>

**Problem**: The previous solution required manual tokenizer pre-download. If tokenizer was not cached, the offline mode would fail during benchmark execution.

**Solution Implemented**:

1. **Added `_ensure_tokenizer_cached()` method** to `DirectBenchmarkController` (lines 109-173):
   - Automatically checks if tokenizer is cached in `~/.cache/huggingface/hub/`
   - If not cached, attempts to download using proxy (if configured)
   - Downloads via subprocess with isolated environment
   - Timeout: 120 seconds
   - Returns True if cached or successfully downloaded, False otherwise

2. **Integrated into `run_benchmark()` method** (lines 304-308):
   - Automatically called before each benchmark execution
   - Gracefully continues even if pre-download fails (offline mode will use any existing cache)
   - Provides clear logging of download status

3. **Key Features**:
   - **Intelligent cache detection**: Uses HuggingFace cache naming convention (`models--org--model-name`)
   - **Proxy support**: Automatically uses `HTTPS_PROXY` environment variable from parent process
   - **Isolated execution**: Downloads in subprocess to avoid breaking Kubernetes API calls
   - **Graceful degradation**: Continues with offline mode even if download fails
   - **Clear logging**: Reports cache status, download progress, and any failures

**Test Results**:
```
[Tokenizer] Not cached, downloading: meta-llama/Llama-3.2-3B-Instruct
[Tokenizer] Download timeout after 120s
[Benchmark] Failed to ensure tokenizer is cached: meta-llama/Llama-3.2-3B-Instruct
[Benchmark] Continuing anyway, offline mode may fail if tokenizer not cached
```

**Answer to User's Question**: Yes, without proxy configuration, the download will timeout after 120 seconds. However:
- System gracefully continues with offline mode enabled
- If tokenizer happens to be cached from previous runs, it will still work
- The solution attempts automatic download but doesn't block execution on failure

**Recommended Usage**:
For first-time use with a new model tokenizer, ensure proxy is configured in environment:
```bash
export HTTPS_PROXY=http://127.0.0.1:1081
```
Then run the autotuner. Subsequent runs will use the cached tokenizer without needing proxy.

**Files Modified**:
- `src/controllers/direct_benchmark_controller.py:109-173` - Added tokenizer caching method
- `src/controllers/direct_benchmark_controller.py:304-308` - Integrated into benchmark workflow

</details>


---

