

## SLO-Aware Objective Scoring with Exponential Penalties

> Design a sophisticated objective scoring algorithm with SLO considerations, making scores increase steeply at the edge of SLO boundary violations.

<details>
<summary>Designed and implemented sophisticated SLO-aware scoring algorithm with exponential penalties and tiered enforcement</summary>

### Requirements Clarification

Used interactive questionnaire to gather requirements:

**SLO Metrics Selected:**
- Latency SLOs (P50, P90, P99 percentiles)
- TTFT SLO (Time to First Token)

**Configuration Approach:** Per-task configuration (each task specifies its own SLO thresholds)

**Penalty Curve:** Exponential (e^x) for smooth but rapidly increasing penalties near boundaries

**Violation Handling:** Tiered approach:
- Minor violations: Heavy penalty only
- Severe violations (beyond fail_ratio): Hard fail with experiment marked as failed

### Algorithm Design

**Core Formula:**
```
final_score = base_objective_score √ó (1 + total_penalty)

Where:
  penalty(metric) = weight √ó exp(violation_ratio / steepness)
  violation_ratio = (actual_value - threshold) / threshold  # Normalized
  total_penalty = Œ£ penalty(metric) for all violated metrics
```

**Key Parameters:**
- `threshold`: Maximum allowed value (in seconds)
- `weight`: Penalty multiplier for the metric (higher = more important)
- `hard_fail`: Enable hard failure enforcement for severe violations
- `fail_ratio`: Violation percentage threshold for hard fail (e.g., 0.2 = 20% over)
- `steepness`: Controls exponential curve slope (lower = steeper, default: 0.1)

**Tiered Enforcement Logic:**
1. If `violation_ratio > fail_ratio` AND `hard_fail=true` ‚Üí Mark experiment as FAILED (score = ‚àû)
2. If `violation_ratio ‚â§ fail_ratio` ‚Üí Apply exponential penalty to score
3. Violations classified by severity:
   - `MINOR`: violation_ratio ‚â§ 0.2 (‚â§20% over)
   - `SEVERE`: violation_ratio > 0.2 (>20% over)
   - `HARD_FAIL`: violation_ratio > fail_ratio with hard_fail enabled

### Implementation

#### 1. Backend Algorithm (`src/utils/optimizer.py`)

**Added Function: `calculate_slo_penalty()`**
```python
def calculate_slo_penalty(
    metrics: Dict[str, Any],
    slo_config: Optional[Dict[str, Any]] = None
) -> Tuple[float, bool, Dict[str, Any]]:
    """Calculate SLO penalty with exponential curve near boundaries.

    Returns:
        (penalty_multiplier, is_hard_failure, violation_details)
    """
```

**Features:**
- Processes latency SLOs for P50/P90/P99 percentiles
- Processes TTFT SLO
- Applies exponential penalty: `weight √ó exp(violation_ratio / steepness)`
- Detects hard failure conditions
- Returns detailed violation information per metric

**Enhanced Function: `calculate_objective_score()`**
```python
def calculate_objective_score(
    results: Dict[str, Any],
    objective: str = "minimize_latency",
    slo_config: Optional[Dict[str, Any]] = None
) -> float:
```

**Changes:**
- Added optional `slo_config` parameter
- Calculates base score from objective
- Applies SLO penalties if configured
- Returns `inf` for hard failures
- Logs detailed violation information

#### 2. Orchestrator Integration (`src/orchestrator.py`)

**Modified: `run_experiment()` method**

Direct benchmark path (lines 176-198):
```python
# Get SLO configuration from task if present
slo_config = task.get("slo")

# Calculate objective score with SLO penalties
score = calculate_objective_score(metrics, task["optimization"]["objective"], slo_config)

# Check if this is a hard SLO failure (score = inf/-inf)
is_slo_failure = (score == float("inf") or score == float("-inf"))

if is_slo_failure:
    experiment_result["status"] = "failed"
    experiment_result["slo_violation"] = True
    print(f"Experiment {experiment_id} FAILED due to hard SLO violation")
else:
    experiment_result["status"] = "success"
```

K8s BenchmarkJob path: Applied identical logic (lines 220-244)

#### 3. Task Configuration Schema

**Example: `examples/docker_task_with_slo.json`**
```json
{
  "task_name": "docker-slo-aware-tune",
  "optimization": {
    "strategy": "grid_search",
    "objective": "minimize_latency"
  },
  "slo": {
    "latency": {
      "p50": {
        "threshold": 2.0,
        "weight": 1.0,
        "hard_fail": false
      },
      "p90": {
        "threshold": 5.0,
        "weight": 2.0,
        "hard_fail": true,
        "fail_ratio": 0.2
      },
      "p99": {
        "threshold": 10.0,
        "weight": 3.0,
        "hard_fail": true,
        "fail_ratio": 0.5
      }
    },
    "ttft": {
      "threshold": 1.0,
      "weight": 2.0,
      "hard_fail": false
    },
    "steepness": 0.1
  }
}
```

#### 4. Frontend TypeScript Types (`frontend/src/types/api.ts`)

**Added Interfaces:**
```typescript
export interface SLOMetricConfig {
  threshold: number;
  weight?: number;
  hard_fail?: boolean;
  fail_ratio?: number;
}

export interface SLOLatencyConfig {
  p50?: SLOMetricConfig;
  p90?: SLOMetricConfig;
  p99?: SLOMetricConfig;
}

export interface SLOConfig {
  latency?: SLOLatencyConfig;
  ttft?: SLOMetricConfig;
  steepness?: number;
}
```

**Extended Existing Interfaces:**
- `Task`: Added `slo?: SLOConfig`
- `Experiment`: Added `slo_violation?: boolean`
- `TaskCreate`: Added `slo?: SLOConfig`

#### 5. Frontend UI - New Task Form (`frontend/src/pages/NewTask.tsx`)

**Added State Variables (18 new states):**
```typescript
const [enableSLO, setEnableSLO] = useState(false);
const [sloP50Threshold, setSloP50Threshold] = useState('2.0');
const [sloP50Weight, setSloP50Weight] = useState('1.0');
// ... (P90, P99, TTFT configurations)
const [sloSteepness, setSloSteepness] = useState('0.1');
```

**Added SLO Configuration Section:**
- Toggle to enable SLO configuration
- Collapsible form with sections for each metric:
  - **P50 Latency**: Threshold, weight (soft penalty only)
  - **P90 Latency**: Threshold, weight, hard fail checkbox, fail_ratio
  - **P99 Latency**: Threshold, weight, hard fail checkbox, fail_ratio
  - **TTFT**: Threshold, weight (soft penalty only)
  - **Steepness**: Global parameter with explanation
- Form integrated into `handleSubmit()` to include SLO in task creation

**UI Features:**
- Clear labeling: "Soft Penalty" vs "Tiered Enforcement"
- Inline help text explaining fail_ratio percentages
- Conditional inputs (fail_ratio only enabled when hard_fail is checked)
- Descriptive steepness parameter guidance

#### 6. Frontend UI - Experiments View (`frontend/src/pages/Experiments.tsx`)

**Added SLO Violation Indicator:**
```tsx
<td className="whitespace-nowrap px-3 py-4 text-sm">
  <div className="flex items-center gap-2">
    <span className={`inline-flex rounded-full px-2 text-xs font-semibold leading-5 ${getStatusColor(experiment.status)}`}>
      {experiment.status}
    </span>
    {experiment.slo_violation && (
      <span className="inline-flex items-center rounded-full bg-red-100 px-2 py-0.5 text-xs font-semibold text-red-800"
            title="Hard SLO violation detected">
        <svg><!-- X icon --></svg>
        SLO
      </span>
    )}
  </div>
</td>
```

**Features:**
- Red "SLO" badge next to status for hard violations
- Tooltip: "Hard SLO violation detected"
- Visual distinction from regular failures

### Testing

**Created Comprehensive Test Suite: `test_slo_algorithm.py`**

**Test 1: No SLO Violations**
```
Metrics: P50=1.5s, P90=4.0s (all within bounds)
Result: Penalty multiplier = 1.0 (no penalty)
‚úì PASSED
```

**Test 2: Minor Violation (10% over)**
```
Metrics: P50 = 2.2s (threshold: 2.0s, weight: 1.0, steepness: 0.1)
Violation Ratio: 10%
Penalty: 1.0 √ó exp(0.10 / 0.1) = 2.72
Penalty Multiplier: 3.72x
Score Increase: 271.83%
Severity: MINOR
‚úì PASSED
```

**Test 3: Severe Violation (25% over)**
```
Metrics: P90 = 6.25s (threshold: 5.0s, weight: 2.0, steepness: 0.1)
Violation Ratio: 25%
Penalty: 2.0 √ó exp(0.25 / 0.1) = 24.36
Penalty Multiplier: 25.37x
Severity: SEVERE
‚úì PASSED - Steep exponential penalty applied
```

**Test 4: Hard Failure (30% over fail_ratio)**
```
Metrics: P90 = 6.5s (threshold: 5.0s, fail_ratio: 0.2)
Violation Ratio: 30% > 20% fail_ratio
Result: HARD_FAIL, score = ‚àû
‚úì PASSED - Experiment marked as failed
```

**Test 5: Multiple Cumulative Violations**
```
Metrics:
  P50: 2.30s > 2.00s (+15%, penalty: +4.48)
  P90: 5.50s > 5.00s (+10%, penalty: +5.44)
  P99: 11.00s > 10.00s (+10%, penalty: +8.15)
  TTFT: 1.20s > 1.00s (+20%, penalty: +14.78)

Total Penalty: 32.85
Penalty Multiplier: 33.85x
Score Increase: 3285% üî•
‚úì PASSED - Cumulative penalties applied
```

**Test 6: Steepness Parameter Effect**
```
Metrics: P90 = 6.0s (20% over 5.0s threshold, weight: 2.0)

Steepness 0.05: penalty_multiplier = 110.20x (very steep)
Steepness 0.1:  penalty_multiplier = 15.78x  (recommended)
Steepness 0.2:  penalty_multiplier = 6.44x   (gentler)

‚úì PASSED - Lower steepness = steeper penalties
```

**All Tests Passed:** 6/6 ‚úÖ

**Test Execution:**
```bash
$ python test_slo_algorithm.py
################################################################################
# Test Summary: 6 passed, 0 failed
################################################################################
```

### Documentation

**Created: `docs/SLO_SCORING.md`**

**Contents:**
- Mathematical formulas and derivations
- Configuration parameter reference
- Example scenarios with calculations
- Steepness parameter impact analysis
- Frontend feature guide
- Backend implementation details
- Use cases and design rationale
- Backward compatibility notes
- Future enhancement ideas

**Key Sections:**
- **Mathematical Formula**: Detailed breakdown of penalty calculation
- **Example Scenarios**: 4 scenarios with step-by-step calculations
- **Steepness Impact Table**: Comparison of 0.05 vs 0.1 vs 0.2
- **Design Rationale**: Why exponential over linear, why tiered enforcement
- **Use Cases**: Production constraints, multi-objective optimization, soft boundaries

### Files Modified

**Backend:**
- `src/utils/optimizer.py` (+149 lines)
  - Added `calculate_slo_penalty()` function
  - Enhanced `calculate_objective_score()` with SLO integration
  - Added `math` import for exponential calculations
  - Added `Tuple` type import

- `src/orchestrator.py` (+24 lines, -8 lines)
  - Modified `run_experiment()` to pass SLO config to scorer
  - Added hard failure detection logic
  - Added `slo_violation` flag to experiment results
  - Applied changes to both direct benchmark and K8s paths

**Frontend:**
- `frontend/src/types/api.ts` (+23 lines)
  - Added `SLOMetricConfig`, `SLOLatencyConfig`, `SLOConfig` interfaces
  - Extended `Task`, `Experiment`, `TaskCreate` interfaces

- `frontend/src/pages/NewTask.tsx` (+214 lines)
  - Added 18 state variables for SLO configuration
  - Added complete SLO configuration form section
  - Enhanced `handleSubmit()` to include SLO in task payload

- `frontend/src/pages/Experiments.tsx` (+18 lines, -6 lines)
  - Added SLO violation badge to experiment status column
  - Added conditional rendering for `slo_violation` flag

**Examples:**
- `examples/docker_task_with_slo.json` (new file)
  - Complete task configuration with SLO section
  - Demonstrates P50/P90/P99 latency + TTFT configuration

**Tests:**
- `test_slo_algorithm.py` (new file, 220 lines)
  - 6 comprehensive test cases
  - Validates exponential penalty behavior
  - Tests tiered enforcement boundaries
  - Verifies steepness parameter effects

**Documentation:**
- `docs/SLO_SCORING.md` (new file, 350 lines)
  - Complete feature documentation
  - Mathematical formulas with examples
  - Configuration guide and use cases

### Build Verification

**TypeScript Type Checking:**
```bash
$ cd frontend && npm run type-check
‚úÖ No errors (all types valid)
```

**Frontend Build:**
```bash
$ cd frontend && npm run build
‚úì 998 modules transformed
dist/assets/index-DHWPilzk.js   672.31 kB ‚îÇ gzip: 197.35 kB
‚úì built in 2.88s
```

### Key Features

**Exponential Penalty Curve:**
- Creates steep gradients near SLO boundaries
- Guides optimization away from unsafe configurations
- 10% violation ‚Üí 2.72x penalty
- 20% violation ‚Üí 15.78x penalty
- 50% violation ‚Üí 297.4x penalty

**Tiered Enforcement:**
- **Soft Penalties** (hard_fail=false): Allow exploration slightly over SLO
- **Hard Failures** (violation > fail_ratio): Reject egregious violations
- Configurable per-metric fail_ratio thresholds

**Multi-Metric Cumulative:**
- Penalties sum across all violated metrics
- Example: 4 violations ‚Üí 33.85x total penalty multiplier
- Allows weighting (P99 more important than P50)

**Configurable Steepness:**
- Controls aggressiveness of penalty curve
- Default 0.1 recommended (balanced)
- Lower values (0.05) create steeper penalties
- Higher values (0.2) create gentler curves

**Backward Compatible:**
- Tasks without `slo` configuration work unchanged
- Fully optional feature
- No breaking changes to existing APIs

### Benefits

**For Users:**
- ‚úÖ Enforce production-like SLO constraints during tuning
- ‚úÖ Balance multiple objectives (latency + TTFT)
- ‚úÖ Prevent configurations that violate critical thresholds
- ‚úÖ Visual feedback for SLO violations in UI

**For System:**
- ‚úÖ Mathematically sound exponential penalty function
- ‚úÖ Flexible tiered enforcement (warn vs fail)
- ‚úÖ Per-task configurability
- ‚úÖ Comprehensive test coverage
- ‚úÖ Full TypeScript type safety

**For Development:**
- ‚úÖ Clean separation of concerns (optimizer vs orchestrator)
- ‚úÖ Detailed violation logging for debugging
- ‚úÖ Well-documented with examples
- ‚úÖ Extensible for future SLO types (throughput, error rate)

### Future Enhancements

Potential additions discussed in documentation:
- Throughput SLOs (minimum thresholds)
- Custom penalty functions (polynomial, piecewise)
- SLO violation budgets (allow N% of experiments to violate)
- SLO-aware Bayesian optimization (constrained BO)

</details>

---

## Add TPOT (Time Per Output Token) SLO Support

**User Request:**
> Add TPOT as a SLO item.

**Context:**
After implementing comprehensive SLO scoring with P50/P90/P99 latency and TTFT metrics, user requested adding TPOT (Time Per Output Token) as an additional SLO metric. TPOT measures the average time to generate each output token during inference.

<details>
<summary>Implementation Details</summary>

### Changes Made

**1. Backend - Optimizer Module** (`src/utils/optimizer.py`)

Added TPOT SLO processing in `calculate_slo_penalty()` function (lines 195-228):

```python
# Process TPOT SLO
tpot_slo = slo_config.get("tpot", {})
if tpot_slo:
    threshold = tpot_slo.get("threshold")
    weight = tpot_slo.get("weight", 1.0)
    hard_fail = tpot_slo.get("hard_fail", False)
    fail_ratio = tpot_slo.get("fail_ratio", 0.5)
    
    if threshold is not None:
        actual_value = metrics.get("mean_tpot")
        
        if actual_value is not None and actual_value > threshold:
            violation_ratio = (actual_value - threshold) / threshold
            
            if hard_fail and violation_ratio > fail_ratio:
                is_hard_failure = True
                violation_details["tpot"] = {
                    "threshold": threshold,
                    "actual": actual_value,
                    "violation_ratio": violation_ratio,
                    "severity": "HARD_FAIL"
                }
            else:
                penalty = weight * math.exp(violation_ratio / steepness)
                total_penalty += penalty
                
                severity = "SEVERE" if violation_ratio > 0.2 else "MINOR"
                violation_details["tpot"] = {
                    "threshold": threshold,
                    "actual": actual_value,
                    "violation_ratio": violation_ratio,
                    "penalty": penalty,
                    "severity": severity
                }
```

**Key Features:**
- Uses same exponential penalty formula as other metrics
- Supports all standard SLO parameters (threshold, weight, hard_fail, fail_ratio)
- Reads `mean_tpot` metric from benchmark results
- Consistent violation tracking and severity classification

**2. Frontend - TypeScript Types** (`frontend/src/types/api.ts`)

Updated `SLOConfig` interface to include TPOT (line 20):

```typescript
export interface SLOConfig {
  latency?: SLOLatencyConfig;
  ttft?: SLOMetricConfig;
  tpot?: SLOMetricConfig;  // Added for TPOT
  steepness?: number;
}
```

**3. Frontend - Task Creation Form** (`frontend/src/pages/NewTask.tsx`)

Added TPOT state management (lines 158, 181-182):
```typescript
const [enableTPOT, setEnableTPOT] = useState(false);
const [sloTpotThreshold, setSloTpotThreshold] = useState('0.05');
const [sloTpotWeight, setSloTpotWeight] = useState('2.0');
```

Added TPOT UI section (lines 1007-1045):
```tsx
{/* TPOT */}
<div className="border-b pb-4">
  <div className="flex items-center justify-between mb-3">
    <h3 className="text-sm font-medium text-gray-900">Time Per Output Token (Soft Penalty)</h3>
    <label className="flex items-center cursor-pointer">
      <input
        type="checkbox"
        checked={enableTPOT}
        onChange={(e) => setEnableTPOT(e.target.checked)}
        className="h-4 w-4 text-blue-600 focus:ring-blue-500 border-gray-300 rounded"
      />
      <span className="ml-2 text-xs text-gray-600">Enable</span>
    </label>
  </div>
  {enableTPOT && (
    <div className="grid grid-cols-2 gap-4">
      <div>
        <label className="block text-xs text-gray-600 mb-1">Threshold (seconds)</label>
        <input
          type="text"
          value={sloTpotThreshold}
          onChange={(e) => setSloTpotThreshold(e.target.value)}
          className="w-full px-3 py-2 border border-gray-300 rounded-md text-sm focus:outline-none focus:ring-2 focus:ring-blue-500"
          placeholder="0.05"
        />
      </div>
      <div>
        <label className="block text-xs text-gray-600 mb-1">Penalty Weight</label>
        <input
          type="text"
          value={sloTpotWeight}
          onChange={(e) => setSloTpotWeight(e.target.value)}
          className="w-full px-3 py-2 border border-gray-300 rounded-md text-sm focus:outline-none focus:ring-2 focus:ring-blue-500"
          placeholder="2.0"
        />
      </div>
    </div>
  )}
</div>
```

Updated form submission logic to include TPOT (lines 380-387):
```typescript
if (enableTPOT && sloTpotThreshold) {
  slo.tpot = {
    threshold: parseFloat(sloTpotThreshold),
    ...(sloTpotWeight && { weight: parseFloat(sloTpotWeight) }),
    hard_fail: false,
  };
}
```

**4. Documentation** (`docs/SLO_SCORING.md`)

Updated documentation to include TPOT:
- Added TPOT to overview multi-metric support list
- Added TPOT to full configuration example
- Added TPOT to multi-objective optimization use case
- Updated test coverage section

**5. Example Configuration** (`examples/docker_task_with_slo.json`)

Added TPOT section to example task:
```json
"tpot": {
  "threshold": 0.05,
  "weight": 2.0,
  "hard_fail": false
}
```

### Testing

Created comprehensive test suite in `test_tpot_slo.py`:

**Test 1: TPOT SLO Violation**
```
Metrics: TPOT = 0.06s (threshold: 0.05s)
Violation Ratio: 20.00%
Penalty Multiplier: 15.7781x
‚úì PASSED - TPOT violation detected and penalized
```

**Test 2: TPOT + TTFT Combined**
```
Metrics:
  TTFT = 1.2s (threshold: 1.0s)
  TPOT = 0.055s (threshold: 0.05s)
Penalty Multiplier: 19.8555x
Violations: ['ttft', 'tpot']
‚úì PASSED - Both TTFT and TPOT violations tracked
```

**Test 3: TPOT Within Bounds**
```
Metrics: TPOT = 0.04s (threshold: 0.05s)
Penalty Multiplier: 1.0000
‚úì PASSED - No violation when within bounds
```

**All Tests Passed: 3/3** ‚úÖ

**Existing Tests Still Pass:**
- `test_slo_algorithm.py`: 6/6 passed ‚úÖ
- `test_slo_optional_fields.py`: 7/7 passed ‚úÖ

### Build Verification

**Frontend Build:**
```bash
$ cd frontend && npm run build
‚úì 998 modules transformed
dist/assets/index-QrLM10Dq.js   675.54 kB ‚îÇ gzip: 197.76 kB
‚úì built in 2.89s
```

### TPOT Penalty Behavior

With default configuration (threshold=0.05s, weight=2.0, steepness=0.1):

| Actual TPOT | Violation | Penalty Multiplier | Score Impact |
|-------------|-----------|-------------------|--------------|
| 0.04s       | 0%        | 1.00x            | No penalty   |
| 0.05s       | 0%        | 1.00x            | No penalty   |
| 0.055s      | 10%       | 3.72x            | 272% worse   |
| 0.06s       | 20%       | 15.78x           | 1478% worse  |
| 0.075s      | 50%       | 595.5x           | 59,450% worse|

The exponential curve creates steep penalties for TPOT violations, encouraging configurations that maintain low per-token generation times.

### Integration Notes

**Metric Source:**
- TPOT values come from genai-bench benchmark results
- Stored in experiment metrics as `mean_tpot` field
- Typically in range 0.01s - 0.10s for modern LLMs

**Default Threshold:**
- Frontend default: 0.05s (50ms per token)
- Reasonable for production inference workloads
- Equals ~20 tokens/second throughput

**Common Use Cases:**

1. **Streaming Applications:**
```json
"slo": {
  "tpot": {"threshold": 0.05, "weight": 3.0}
}
```
Critical for real-time streaming where per-token latency matters.

2. **Combined with TTFT:**
```json
"slo": {
  "ttft": {"threshold": 1.0, "weight": 2.0},
  "tpot": {"threshold": 0.05, "weight": 2.0}
}
```
Optimize both first token latency and subsequent token generation speed.

3. **Balanced Optimization:**
```json
"slo": {
  "latency": {"p90": {"threshold": 5.0, "weight": 1.5}},
  "ttft": {"threshold": 1.0, "weight": 2.0},
  "tpot": {"threshold": 0.05, "weight": 2.0}
}
```
Multi-metric optimization considering end-to-end latency, initial response time, and sustained generation speed.

### Files Modified

**Backend:**
- `src/utils/optimizer.py` (+34 lines)
  - Added TPOT processing in calculate_slo_penalty()

**Frontend:**
- `frontend/src/types/api.ts` (+1 line)
  - Added tpot to SLOConfig interface
- `frontend/src/pages/NewTask.tsx` (+51 lines, lines 158, 181-182, 380-387, 1007-1045)
  - Added TPOT state variables
  - Added TPOT UI section
  - Added TPOT to form submission

**Documentation:**
- `docs/SLO_SCORING.md` (+8 lines)
  - Updated overview and examples

**Examples:**
- `examples/docker_task_with_slo.json` (+5 lines)
  - Added TPOT configuration

**Tests:**
- `test_tpot_slo.py` (new file, 150 lines)
  - 3 comprehensive test cases

### Benefits

**For Users:**
- ‚úÖ Fine-grained control over token generation speed
- ‚úÖ Enforce streaming performance requirements
- ‚úÖ Optimize for sustained throughput vs burst performance

**For System:**
- ‚úÖ Complete metric coverage (latency, TTFT, TPOT)
- ‚úÖ Consistent exponential penalty behavior
- ‚úÖ Follows same pattern as existing SLO metrics

**Technical Advantages:**
- ‚úÖ TPOT is independent of request size (normalized metric)
- ‚úÖ Better indicator of decoding efficiency than total latency
- ‚úÖ Useful for comparing configurations across different request patterns

</details>

---

## Reorder SLO Configuration UI (TTFT/TPOT First)

**User Request:**
> Reorder SLO Configuration UI, put TTFT & TPOT at first.

**Context:**
User requested reordering the SLO configuration form to prioritize TTFT (Time to First Token) and TPOT (Time Per Output Token) metrics, moving them before the latency percentiles (P50/P90/P99). This improves UX by placing frequently-used token-level metrics at the top.

<details>
<summary>Implementation Details</summary>

### Changes Made

**Frontend - Task Creation Form** (`frontend/src/pages/NewTask.tsx`)

Reordered SLO metric sections (lines 805-1046):

**New Order:**
1. **TTFT** (Time to First Token) - Lines 807-845
2. **TPOT** (Time Per Output Token) - Lines 847-885
3. **P50 Latency** - Lines 887-925
4. **P90 Latency** - Lines 927-985
5. **P99 Latency** - Lines 987-1045
6. **Steepness** - Lines 1047+ (unchanged position)

**Rationale:**
- TTFT and TPOT are token-level metrics that are often more relevant for streaming/real-time applications
- Latency percentiles (P50/P90/P99) are broader end-to-end metrics
- Grouping related metrics improves cognitive load
- Token-level metrics typically need more frequent tuning

### Build Verification

**Frontend Build:**
```bash
$ cd frontend && npm run build
‚úì 998 modules transformed
dist/assets/index-QrLM10Dq.js   675.54 kB ‚îÇ gzip: 197.76 kB
‚úì built in 2.89s
```

### UI Improvements

**Before:**
```
SLO Configuration
‚îú‚îÄ‚îÄ P50 Latency
‚îú‚îÄ‚îÄ P90 Latency
‚îú‚îÄ‚îÄ P99 Latency
‚îú‚îÄ‚îÄ TTFT
‚îú‚îÄ‚îÄ TPOT
‚îî‚îÄ‚îÄ Steepness
```

**After:**
```
SLO Configuration
‚îú‚îÄ‚îÄ TTFT (Time to First Token)      ‚Üê Moved up
‚îú‚îÄ‚îÄ TPOT (Time Per Output Token)    ‚Üê Moved up
‚îú‚îÄ‚îÄ P50 Latency
‚îú‚îÄ‚îÄ P90 Latency
‚îú‚îÄ‚îÄ P99 Latency
‚îî‚îÄ‚îÄ Steepness
```

### Benefits

**For Users:**
- ‚úÖ More frequently-used metrics appear first
- ‚úÖ Token-level metrics grouped together logically
- ‚úÖ Reduced scrolling for common use cases
- ‚úÖ Clearer distinction between token metrics and latency percentiles

**UX Considerations:**
- Token metrics (TTFT, TPOT) are simpler to configure (threshold + weight only)
- Latency percentiles have additional complexity (hard_fail, fail_ratio options)
- New users typically start with token metrics before advanced latency constraints

### Files Modified

**Frontend:**
- `frontend/src/pages/NewTask.tsx` (~260 lines reordered)
  - Moved TTFT section from line 967 to line 807
  - Moved TPOT section from line 1007 to line 847
  - All functionality preserved, only position changed

### Technical Notes

**No Breaking Changes:**
- Reordering is purely visual/UX improvement
- Form submission logic unchanged
- API payload structure unchanged
- Backend receives identical data regardless of UI order
- All state management and validation preserved

</details>

---

## Deep Dive: aiconfigurator Integration Analysis

**User Request:**
> Add https://github.com/ai-dynamo/aiconfigurator as a submodule.
> Deep dive the project aiconfigurator, mine good features adapted for our development target.

**Context:**
User requested adding NVIDIA's aiconfigurator as a submodule and performing a comprehensive analysis to identify features that could benefit the inference-autotuner project.

<details>
<summary>Comprehensive Analysis: 10 High-Value Features Identified from NVIDIA aiconfigurator</summary>

### Submodule Addition

```bash
$ git submodule add https://github.com/ai-dynamo/aiconfigurator
‚úì Submodule added on branch test/aiconfigurator
```

### What is aiconfigurator?

**NVIDIA's AI System** for automatically optimizing LLM inference deployments:
- Searches thousands of configuration combinations (parallelism, quantization, serving architecture)
- Achieves **1.7x-2x performance improvements** under SLA constraints
- **Core Innovation**: Disaggregated inference (separate prefill/decode worker pools)

### Deep Dive Results

**Exploration Coverage:**
- 40+ code modules analyzed
- 50+ configuration options documented
- 5 design patterns identified
- 4 comprehensive documents created (1,970 lines total)

### Top 10 Features for Inference-Autotuner

| Feature | Value | Priority | Status |
|---------|-------|----------|---------|
| 1. Pareto Frontier Analysis | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **HIGH** | Not implemented |
| 2. Layered Config Factory | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **HIGH** | Partial |
| 3. Exponential Penalties | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | **HIGH** | ‚úÖ Implemented! |
| 4. Backend Strategy Pattern | ‚≠ê‚≠ê‚≠ê‚≠ê | **HIGH** | ‚úÖ Similar exists |
| 5. Performance Modeling | ‚≠ê‚≠ê‚≠ê‚≠ê | MEDIUM | Not implemented |
| 6. Config Profiles | ‚≠ê‚≠ê‚≠ê‚≠ê | MEDIUM | ‚úÖ Frontend |
| 7. Heterogeneous Deploy | ‚≠ê‚≠ê‚≠ê | LOW | Not implemented |
| 8. Multi-Experiment Compare | ‚≠ê‚≠ê‚≠ê | MEDIUM | Partial |
| 9. Replica Scaling | ‚≠ê‚≠ê‚≠ê | LOW | Not implemented |
| 10. Template Generation | ‚≠ê‚≠ê‚≠ê | LOW | Not implemented |

### Quick Wins

**1. Pareto Analysis (2-3 days, High Value)**
- Multi-objective optimization finding non-dominated configurations
- Reveals optimal trade-offs between throughput/latency/cost
- Implementation: Add to `src/utils/pareto.py`

**2. Layered SLO Configuration (1-2 days, High Value)**
- Compose SLO configs from layers: Base ‚Üí Model-specific ‚Üí Task-specific
- Pattern from `aiconfigurator/sdk/task.py`
- Implementation: New `src/utils/config_factory.py`

**3. SLO Profile Presets (1 day, Medium Value)**
- Pre-built templates: strict_latency, high_throughput, balanced
- Enhance existing frontend presets

### Integration Phases

**Phase 1: Core Enhancements (1-2 weeks)**
- Implement Pareto frontier analysis
- Add layered SLO configuration factory
- Create SLO profile presets
- Add Pareto visualization to frontend

**Phase 2: Performance Modeling (2-3 weeks)**
- Build performance database from historical experiments
- Implement operation-based modeling
- Enable "what-if" analysis

**Phase 3: Advanced Features (3-4 weeks)**
- Multi-experiment comparison dashboard
- Template-based config generation
- Heterogeneous deployment support

### Code Reuse Opportunities

**Direct Adaptation:**
1. `aiconfigurator/sdk/pareto_analysis.py` ‚Üí `src/utils/pareto.py`
2. `aiconfigurator/sdk/task.py` ‚Üí `src/utils/config_factory.py`
3. Backend abstraction patterns

**Design Patterns:**
- Layered config factory for SLO composition
- Operation-based modeling for performance prediction
- Template generation for deployment artifacts

### Key Architectural Insights

**Recommended Layering:**
```
User Interface (Frontend/CLI)
    ‚Üì
Task Configuration Layer (with profiles, layers)
    ‚Üì
Orchestration Layer (orchestrator.py)
    ‚Üì
Execution Layer (controllers/)
    ‚Üì
Backend Layer (frameworks: sglang, vllm)
```

### Documentation Created

1. **AICONFIGURATOR_INDEX.md** (244 lines) - Navigation guide
2. **AICONFIGURATOR_SUMMARY.md** (283 lines) - Quick reference
3. **AICONFIGURATOR_ANALYSIS.md** (1,123 lines) - Comprehensive technical analysis
4. **AICONFIGURATOR_MANIFEST.txt** (320 lines) - Project metadata
5. **docs/AICONFIGURATOR_FEATURES.md** - Integration roadmap with implementation details

### Immediate Action Items

**High Priority:**
1. Implement Pareto frontier analysis (reveals optimal trade-offs)
2. Add layered SLO configuration (improves flexibility)
3. Create SLO profile presets (better UX)

**Long-term:**
1. Performance modeling database (reduce benchmarking cost)
2. Template-based deployment generation (faster deployment)
3. Heterogeneous hardware support (cost optimization)

### Summary

aiconfigurator provides proven patterns for multi-objective optimization, flexible configuration composition, and performance prediction. The most valuable takeaway is **Pareto analysis** for discovering optimal configurations with conflicting objectives - this is not yet implemented in inference-autotuner and would provide significant value.

</details>

---

