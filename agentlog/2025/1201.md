

> Implement per-batch SLO filtering for benchmark results

<details>
<summary>Added SLO compliance checking at individual batch (concurrency level) granularity with filtering before metric aggregation</summary>

**Requirements:**
1. Allow partial batch failures in genai-bench without failing entire experiment
2. Check SLO compliance at individual batch (concurrency level) granularity
3. Filter out batches that violate SLO constraints
4. Calculate max-throughput only from SLO-compliant batches
5. Log which batches passed/failed SLO checks

**Problem Background:**
Previously, SLO constraints were applied to aggregated metrics across all batches. This meant:
- No per-batch SLO filtering
- Max throughput calculated from all batches regardless of SLO violations
- If any batch violated hard SLO constraints, entire experiment failed

**New Behavior:**
- Each batch (concurrency level) checked individually against SLO constraints
- Batches violating SLO constraints are filtered out before aggregation
- Max throughput calculated only from SLO-compliant batches
- Experiment succeeds if at least one batch passes SLO (graceful degradation)
- Clear logging of which batches passed/failed SLO checks

**Implementation:**

1. **Created `check_batch_slo_compliance()` function** in `src/utils/optimizer.py`:
   ```python
   def check_batch_slo_compliance(batch_metrics: Dict[str, Any], slo_config: Optional[Dict[str, Any]] = None) -> Tuple[bool, Dict[str, Any]]:
       """Check if a single batch (concurrency level) meets SLO requirements.
       
       Returns:
           Tuple of (is_compliant, violation_details)
       """
   ```
   - Checks latency (p50, p90, p99), TTFT, and TPOT thresholds
   - Only enforces hard_fail violations (soft penalties handled elsewhere)
   - Returns compliance status and detailed violation information

2. **Modified `direct_benchmark_controller.py`** to filter batches:
   - Added SLO filtering logic before metric aggregation (lines 573-595)
   - Filters out non-compliant batches when `slo_config` provided
   - Falls back to all batches with penalties if no batches pass SLO
   - Logs compliance results for each batch

3. **Updated `orchestrator.py`** to pass SLO config:
   - Merged `slo_config` into `benchmark_config` before calling benchmark controller
   - Ensures SLO config is available for per-batch filtering

**Example Log Output:**
```
[Benchmark] Filtering 4 batches by SLO compliance...
[Benchmark] ✗ Batch concurrency=8 violated SLO: {'p90': {'threshold': 5.0, 'actual': 6.2, 'violation_ratio': 0.24, 'type': 'HARD_FAIL'}}
[Benchmark] ✓ 3/4 batches passed SLO
[Benchmark] Max throughput: 145.2 req/s (from 3 SLO-compliant batches)
```

**Behavior Changes:**

| Scenario | Before | After |
|----------|--------|-------|
| 4 batches, 3 pass SLO | All 4 used for max throughput | Only 3 SLO-compliant used |
| All batches fail SLO | Experiment fails | Uses all with penalties |
| Partial batch failure + SLO | May use failing batches | Filters failed batches first |

**Files Modified:**
- `src/utils/optimizer.py` (+133 lines): Added `check_batch_slo_compliance()`
- `src/controllers/direct_benchmark_controller.py` (+48 lines): Added SLO filtering logic
- `src/orchestrator.py` (+7 lines): Pass slo_config to benchmark controller

**Status:**
- ✅ Per-batch SLO checking implemented
- ✅ SLO filtering before aggregation
- ✅ Graceful handling when no batches pass
- ✅ Detailed logging of compliance results
- ⏭️ Ready for testing with real experiments

</details>

---

> Handle partial batch success gracefully (OOM tolerance)

<details>
<summary>Modified error handling to allow experiments to succeed even when some batches fail, enabling graceful degradation</summary>

**Problem:**
When genai-bench exits with non-zero code (e.g., OOM at high concurrency), the entire experiment would fail even if some batches completed successfully.

**Example Scenario:**
- Concurrency levels: [1, 4, 6, 8]
- Batches 1 and 4 complete successfully
- Batch 6 causes OOM and kills genai-bench
- **Before**: Entire experiment marked as FAILED, no results saved
- **After**: Experiment succeeds with results from batches 1 and 4

**Solution Implemented:**

Three-layer fault tolerance in result parsing:

1. **Process-level tolerance** (`direct_benchmark_controller.py` lines 489-496):
   ```python
   # Even if genai-bench exits with error, try to parse partial results
   if result_returncode != 0:
       print(f"[Benchmark] WARNING: genai-bench exited with non-zero code: {result_returncode}")
       print(f"[Benchmark] Will attempt to parse partial results from successful batches...")
   
   # Parse results from output directory (may contain partial results)
   metrics = self._parse_results(output_dir, slo_config=benchmark_config.get("slo_config"))
   ```

2. **File-level tolerance** (lines 564-575):
   ```python
   for result_file in result_files:
       try:
           with open(result_file, "r") as f:
               data = json.load(f)
           if "aggregated_metrics" in data:
               all_metrics.append(data["aggregated_metrics"])
       except Exception as e:
           print(f"[Benchmark] Error parsing {result_file.name}: {e}")
           continue  # Skip bad files, keep processing
   ```

3. **Empty results check** (lines 601-608):
   ```python
   if not all_metrics:
       print(f"[Benchmark] ✗ No valid metrics found in results!")
       return None  # Only fail if NO results exist
   ```

**Behavior Changes:**

| Event | Before | After |
|-------|--------|-------|
| OOM at concurrency=8 | Experiment FAILED | Uses results from 1,4,6 |
| genai-bench crashes | No results parsed | Parses partial results |
| Some result files corrupted | Parsing fails | Skip bad files, keep good |
| All batches fail | Experiment FAILED | Still FAILED (no data) |

**Integration with SLO Filtering:**

The two features work together:
1. Parse all successful batches (even if some failed)
2. Filter parsed batches by SLO compliance
3. Aggregate only SLO-compliant successful batches
4. Experiment succeeds if at least one batch passes both checks

**Example Combined Flow:**
```
genai-bench runs 4 concurrency levels [1, 4, 6, 8]
├─ Concurrency 1: ✓ Success, ✓ SLO compliant
├─ Concurrency 4: ✓ Success, ✗ SLO violation (filtered)
├─ Concurrency 6: ✓ Success, ✓ SLO compliant  
└─ Concurrency 8: ✗ OOM (no results)

Result: Experiment SUCCESS with 2 compliant batches (1, 6)
```

**Files Modified:**
- `src/controllers/direct_benchmark_controller.py`:
  - Modified error handling to attempt parsing even on non-zero exit codes
  - Added try-catch around individual file parsing
  - Changed failure condition to only fail if NO results exist

**Status:**
- ✅ Partial batch success handling implemented
- ✅ OOM tolerance added
- ✅ Individual file parsing errors handled gracefully
- ✅ Integrated with SLO filtering
- ⏭️ Testing with OOM scenarios

</details>



---

> Fix timezone display issue in frontend - API datetime serialization with UTC timezone marker

<details>
<summary>Fixed timezone serialization by adding Pydantic field serializers to append 'Z' suffix to UTC datetime fields</summary>

**Problem:**
- Frontend TimezoneContext expects API to return datetime with timezone indicator
- SQLite DateTime columns don't preserve timezone information
- FastAPI/Pydantic was serializing datetime as ISO format without timezone suffix
- Example: `"2025-12-01T02:53:39.435199"` (no timezone indicator)
- JavaScript `new Date()` parses strings without timezone as local time, not UTC
- This caused incorrect time display in the frontend

**Root Cause Analysis:**
1. Database stores datetime with `datetime.utcnow()` (UTC, but timezone-naive)
2. SQLAlchemy reads datetime objects without timezone info
3. Pydantic serializes datetime objects using `.isoformat()` (no 'Z' suffix)
4. JavaScript interprets timezone-less datetime as local time
5. Frontend tries to convert to Asia/Shanghai, but source is already wrong

**Solution:**
Added Pydantic `@field_serializer` decorators to response schemas to append 'Z' suffix when serializing to JSON:

```python
@field_serializer('created_at', 'started_at', 'completed_at', when_used='json')
def serialize_datetime(self, dt: Optional[datetime]) -> Optional[str]:
    if dt is None:
        return None
    return dt.isoformat() + 'Z'
```

**Implementation Steps:**

1. **Modified `/root/work/inference-autotuner/src/web/schemas/__init__.py`**:
   - Added `field_serializer` import from pydantic
   - Added datetime serializer method to `TaskResponse` class
   - Added datetime serializer method to `TaskListResponse` class  
   - Added datetime serializer method to `ExperimentResponse` class

2. **Created fix script** (`/tmp/fix_datetime_v2.py`):
   - Automated insertion of serializer methods
   - Handled file permissions with sudo

3. **Verified fix**:
   - Restarted web server
   - Tested API responses: all datetime fields now have 'Z' suffix
   - Example: `"created_at": "2025-12-01T02:53:39.435199Z"` ✅

**Files Modified:**
- `src/web/schemas/__init__.py` (+21 lines): Added field serializers to 3 response models

**Verification:**
```bash
# Before fix:
curl http://localhost:8000/api/tasks/ | jq '.[0].created_at'
"2025-12-01T02:53:39.435199"  # No timezone indicator

# After fix:
curl http://localhost:8000/api/tasks/ | jq '.[0].created_at'
"2025-12-01T02:53:39.435199Z"  # UTC timezone indicator present ✅
```

**Alternative Approaches Considered:**
1. ❌ CustomORJSONResponse in app.py - Pydantic serializes before it reaches response class
2. ❌ Custom SQLAlchemy type converter - Complex and affects database layer
3. ✅ Pydantic field_serializer - Clean, type-specific, works at serialization layer

**Status:**
- ✅ Timezone serialization fixed
- ✅ All API endpoints return datetime with 'Z' suffix
- ✅ Frontend can now correctly interpret times as UTC
- ✅ Web server restarted with new code
- ⏭️ Frontend should now display correct timezone-converted times

</details>

---

> Transition development diary workflow from master log splitting to direct diary file writing

<details>
<summary>Updated documentation and workflow to write directly to daily diary files instead of maintaining a master agentlog.md file</summary>

**User Request:**
"There is no agentlog.md file, in future we always write agent log in a diary file in agentlog/, understand?"

**Context:**
Previously, the workflow involved:
1. Writing entries to master `agentlog.md` with date markers `## yyyy/mm/dd`
2. Running `scripts/split_agentlog.js` to split into daily diary files
3. This created an extra maintenance step

**New Workflow:**
Write directly to today's diary file at `agentlog/yyyy/mmdd.md` without intermediate master log.

**User Decisions (via AskUserQuestion):**
1. ✅ Delete `agentlog.md` (already deleted)
2. ✅ Keep `scripts/split_agentlog.js` for historical log processing
3. ✅ Auto-select current date (no manual date specification)
4. ✅ Update `/devlog` command to write directly to diary files

**Implementation:**

1. **Updated `.claude/commands/devlog.md`**:
   - Removed references to `agentlog.md` and split script
   - New instruction: "Append entry to today's diary file: `agentlog/yyyy/mmdd.md`"
   - Added guidance: "Create the diary file if it doesn't exist for today"
   - Clarified use of `---` separators between entries

2. **Updated `.claude/commands/meta.md`**:
   - Changed from "learn from `agentlog.md`" 
   - To "learn from `agentlog/` diary files"

3. **Updated `CLAUDE.md` Meta-Instructions (lines 489-506)**:
   - Line 490: "Write directly to today's diary file: `agentlog/yyyy/mmdd.md`"
   - Line 495: "Use `/devlog` command to add entries to today's diary"
   - Rewrote "Development Diary Structure" section:
     - Removed master log references
     - Documented direct diary writing workflow
     - Noted split script as "preserved for historical log processing"

4. **Updated `README.md` (line 578-580)**:
   - Removed: "Master development log with date markers"
   - Simplified to: "Daily development diary (yyyy/mmdd.md format) written directly to files"

**Files Modified:**
- `.claude/commands/devlog.md` - Complete rewrite for direct writing
- `.claude/commands/meta.md` - Updated reference to diary files
- `CLAUDE.md` - Updated Meta-Instructions and Development Diary Structure sections
- `README.md` - Simplified Development History documentation

**Files Preserved:**
- `scripts/split_agentlog.js` (3.2K, executable) - Kept for historical use
- 22 existing diary files in `agentlog/2025/` - Unchanged

**Verification:**
- ✅ All references to `agentlog.md` removed from documentation
- ✅ `/devlog` command describes direct diary file writing
- ✅ CLAUDE.md clearly explains new workflow
- ✅ README.md reflects current state
- ✅ Split script preserved for legacy processing

**New Developer Experience:**
Going forward, Claude will automatically:
1. Write to `agentlog/yyyy/mmdd.md` based on current date
2. Create the file and directory if they don't exist
3. Use `---` separators between multiple entries on same day
4. No need to run split script for daily updates

This simplifies the workflow by eliminating the intermediate master log step while preserving the ability to process historical logs if needed.

</details>
