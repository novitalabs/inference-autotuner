{
  "task_name": "llama-3-1-8b-tuning",
  "description": "Optimize Llama 3.1 8B inference parameters",
  "model": {
    "id_or_path": "llama-3-1-8b-instruct",
    "namespace": "autotuner"
  },
  "base_runtime": "llama-3-2-1b-instruct-rt",
  "parameters": {
    "tp_size": {
      "type": "choice",
      "values": [1]
    },
    "mem_frac": {
      "type": "choice",
      "values": [0.8, 0.85, 0.9]
    },
    "max_total_tokens": {
      "type": "choice",
      "values": [4096, 8192]
    },
    "schedule_policy": {
      "type": "choice",
      "values": ["lpm", "random", "fcfs"]
    }
  },
  "optimization": {
    "strategy": "grid_search",
    "objective": "minimize_latency",
    "max_iterations": 18,
    "timeout_per_iteration": 900
  },
  "benchmark": {
    "task": "text-to-text",
    "traffic_scenarios": [
      "D(100,100)",
      "D(1000,100)"
    ],
    "num_concurrency": [1, 4, 16],
    "max_time_per_iteration": 15,
    "max_requests_per_iteration": 100,
    "additional_params": {
      "temperature": "0.0"
    }
  }
}
