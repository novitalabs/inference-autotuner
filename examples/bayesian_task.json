{
  "task_name": "bayesian-llama3-tune",
  "description": "Bayesian optimization example for Llama 3.2-1B inference tuning. Uses intelligent search to find optimal configuration in ~30 experiments instead of 160+ for grid search.",
  "model": {
    "id_or_path": "llama-3-2-1b-instruct",
    "namespace": "autotuner"
  },
  "base_runtime": "sglang",
  "runtime_image_tag": "v0.5.2-cu126",
  "parameters": {
    "tp-size": [1, 2],
    "mem-fraction-static": [0.7, 0.75, 0.8, 0.85, 0.9],
    "schedule-policy": ["lpm", "fcfs"],
    "chunked-prefill-size": [512, 1024, 2048, 4096]
  },
  "optimization": {
    "strategy": "bayesian",
    "objective": "minimize_latency",
    "max_iterations": 30,
    "timeout_per_iteration": 600
  },
  "benchmark": {
    "task": "text-to-text",
    "model_name": "Llama-3.2-1B-Instruct",
    "model_tokenizer": "meta-llama/Llama-3.2-1B-Instruct",
    "traffic_scenarios": ["D(100,100)"],
    "num_concurrency": [4, 8],
    "max_time_per_iteration": 30,
    "max_requests_per_iteration": 100,
    "additional_params": {
      "temperature": 0.0
    }
  },
  "slo": {
    "latency": {
      "p90": {
        "threshold": 5.0,
        "weight": 2.0,
        "hard_fail": true,
        "fail_ratio": 0.2
      }
    },
    "ttft": {
      "threshold": 1.0,
      "weight": 2.0,
      "hard_fail": false
    },
    "steepness": 0.1
  }
}
