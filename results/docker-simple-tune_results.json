{
  "task_name": "docker-simple-tune",
  "strategy": "grid_search",
  "total_experiments": 2,
  "successful_experiments": 2,
  "elapsed_time": 128.85788679122925,
  "best_result": {
    "experiment_id": 2,
    "parameters": {
      "tp-size": 1,
      "mem-fraction-static": 0.8
    },
    "status": "success",
    "metrics": {
      "num_result_files": 2,
      "concurrency_levels": [
        4,
        1
      ],
      "raw_results": [
        {
          "scenario": "D(100,100)",
          "num_concurrency": 4,
          "batch_size": 1,
          "iteration_type": "num_concurrency",
          "run_duration": 3.0056757628917694,
          "mean_output_throughput_tokens_per_s": 1730.0601961793327,
          "mean_input_throughput_tokens_per_s": 1701.4476621655976,
          "mean_total_tokens_throughput_tokens_per_s": 3431.5078583449304,
          "mean_total_chars_per_hour": 50157397.806390256,
          "requests_per_second": 17.300601961793326,
          "error_codes_frequency": {},
          "error_rate": 0,
          "num_error_requests": 0,
          "num_completed_requests": 52,
          "num_requests": 52,
          "stats": {
            "ttft": {
              "min": 0.011742539703845978,
              "max": 0.26349610928446054,
              "mean": 0.04581959448898068,
              "stddev": 0.06004420390343907,
              "sum": 2.3826189134269953,
              "p25": 0.021264498122036457,
              "p50": 0.027980441693216562,
              "p75": 0.03369458741508424,
              "p90": 0.12693287534639225,
              "p95": 0.19588832021690855,
              "p99": 0.26273496785201134
            },
            "tpot": {
              "min": 0.0016091428697109222,
              "max": 0.004043731010622448,
              "mean": 0.0017182420927990686,
              "stddev": 0.00036644888163548094,
              "sum": 0.08934858882555156,
              "p25": 0.0016215999300281207,
              "p50": 0.001634153565674117,
              "p75": 0.0016712439575731152,
              "p90": 0.0017361357723447409,
              "p95": 0.0017693769774691323,
              "p99": 0.0034281404273151773
            },
            "e2e_latency": {
              "min": 0.1785618867725134,
              "max": 0.42280125338584185,
              "mean": 0.21592556167608842,
              "stddev": 0.06610715847078844,
              "sum": 11.228129207156599,
              "p25": 0.18564055324532092,
              "p50": 0.1909139105118811,
              "p75": 0.1995104046072811,
              "p90": 0.298194214515388,
              "p95": 0.41844711694866416,
              "p99": 0.4226723491027951
            },
            "output_latency": {
              "min": 0.1593051441013813,
              "max": 0.4003293700516224,
              "mean": 0.17010596718710774,
              "stddev": 0.03627843928191263,
              "sum": 8.845510293729603,
              "p25": 0.16053839307278395,
              "p50": 0.1617812030017376,
              "p75": 0.1654531517997384,
              "p90": 0.17187744146212935,
              "p95": 0.17516832076944408,
              "p99": 0.33938590230420257
            },
            "output_inference_speed": {
              "min": 247.296369954655,
              "max": 621.4488587825934,
              "mean": 594.8696381599027,
              "stddev": 61.61944967991773,
              "sum": 30933.221184314938,
              "p25": 598.3646504684243,
              "p50": 611.9375966246494,
              "p75": 616.6749185776496,
              "p90": 620.6711587244066,
              "p95": 621.1654001162117,
              "p99": 621.4126095951355
            },
            "num_input_tokens": {
              "min": 95,
              "max": 100,
              "mean": 98.34615384615384,
              "stddev": 1.1748095653308666,
              "sum": 5114,
              "p25": 97.75,
              "p50": 98.0,
              "p75": 99.0,
              "p90": 100.0,
              "p95": 100.0,
              "p99": 100.0
            },
            "num_output_tokens": {
              "min": 100,
              "max": 100,
              "mean": 100.0,
              "stddev": 0.0,
              "sum": 5200,
              "p25": 100.0,
              "p50": 100.0,
              "p75": 100.0,
              "p90": 100.0,
              "p95": 100.0,
              "p99": 100.0
            },
            "total_tokens": {
              "min": 195,
              "max": 200,
              "mean": 198.34615384615384,
              "stddev": 1.1748095653308666,
              "sum": 10314,
              "p25": 197.75,
              "p50": 198.0,
              "p75": 199.0,
              "p90": 200.0,
              "p95": 200.0,
              "p99": 200.0
            },
            "input_throughput": {
              "min": 364.33175526080345,
              "max": 8345.724389409774,
              "mean": 3846.5724401047637,
              "stddev": 1940.820954921548,
              "sum": 200021.76688544772,
              "p25": 2943.5514298465278,
              "p50": 3517.718045740773,
              "p75": 4585.774827205975,
              "p90": 7376.0324599542555,
              "p95": 8048.303684886858,
              "p99": 8299.675796735595
            },
            "output_throughput": {
              "min": 247.296369954655,
              "max": 621.4488587825934,
              "mean": 594.8696381599027,
              "stddev": 61.61944967991773,
              "sum": 30933.221184314938,
              "p25": 598.3646504684243,
              "p50": 611.9375966246494,
              "p75": 616.6749185776497,
              "p90": 620.6711587244065,
              "p95": 621.1654001162117,
              "p99": 621.4126095951355
            }
          }
        },
        {
          "scenario": "D(100,100)",
          "num_concurrency": 1,
          "batch_size": 1,
          "iteration_type": "num_concurrency",
          "run_duration": 9.093830207362771,
          "mean_output_throughput_tokens_per_s": 604.8056621451932,
          "mean_input_throughput_tokens_per_s": 596.0084888776267,
          "mean_total_tokens_throughput_tokens_per_s": 1200.81415102282,
          "mean_total_chars_per_hour": 17551967.10913088,
          "requests_per_second": 6.048056621451932,
          "error_codes_frequency": {},
          "error_rate": 0,
          "num_error_requests": 0,
          "num_completed_requests": 55,
          "num_requests": 55,
          "stats": {
            "ttft": {
              "min": 0.010148720815777779,
              "max": 0.03353619482368231,
              "mean": 0.011515504070980983,
              "stddev": 0.0033043659980104877,
              "sum": 0.633352723903954,
              "p25": 0.010281766764819622,
              "p50": 0.010465477593243122,
              "p75": 0.01126451464369893,
              "p90": 0.013171770796179772,
              "p95": 0.014313541539013383,
              "p99": 0.024850562494248166
            },
            "tpot": {
              "min": 0.0015207312982341255,
              "max": 0.0018613767349208244,
              "mean": 0.0015327113831913504,
              "stddev": 4.493010490665829e-05,
              "sum": 0.08429912607552427,
              "p25": 0.0015248116388013868,
              "p50": 0.0015252597272546605,
              "p75": 0.0015259386425969575,
              "p90": 0.001533680691412001,
              "p95": 0.001537989540909878,
              "p99": 0.0016904403570324486
            },
            "e2e_latency": {
              "min": 0.16107304487377405,
              "max": 0.19449578132480383,
              "mean": 0.16325393100692467,
              "stddev": 0.005373834061503619,
              "sum": 8.978966205380857,
              "p25": 0.16131580481305718,
              "p50": 0.16160020045936108,
              "p75": 0.16267037438228726,
              "p90": 0.16474761608988048,
              "p95": 0.16699957717210054,
              "p99": 0.18887589981779457
            },
            "output_latency": {
              "min": 0.15055239852517843,
              "max": 0.18427629675716162,
              "mean": 0.1517384269359437,
              "stddev": 0.004448080385759171,
              "sum": 8.345613481476903,
              "p25": 0.1509563522413373,
              "p50": 0.15100071299821138,
              "p75": 0.1510679256170988,
              "p90": 0.15183438844978808,
              "p95": 0.15226096455007793,
              "p99": 0.1673535953462124
            },
            "output_inference_speed": {
              "min": 537.2367566647039,
              "max": 657.5783645415866,
              "mean": 652.9029606870484,
              "stddev": 15.845540919907515,
              "sum": 35909.66283778766,
              "p25": 655.3343445574214,
              "p50": 655.6260433099588,
              "p75": 655.8187087497976,
              "p90": 656.151814760751,
              "p95": 656.3646671053781,
              "p99": 656.9439974905414
            },
            "num_input_tokens": {
              "min": 96,
              "max": 101,
              "mean": 98.54545454545455,
              "stddev": 1.1726920361161526,
              "sum": 5420,
              "p25": 98.0,
              "p50": 99.0,
              "p75": 99.0,
              "p90": 100.0,
              "p95": 100.0,
              "p99": 101.0
            },
            "num_output_tokens": {
              "min": 100,
              "max": 100,
              "mean": 100.0,
              "stddev": 0.0,
              "sum": 5500,
              "p25": 100.0,
              "p50": 100.0,
              "p75": 100.0,
              "p90": 100.0,
              "p95": 100.0,
              "p99": 100.0
            },
            "total_tokens": {
              "min": 196,
              "max": 201,
              "mean": 198.54545454545453,
              "stddev": 1.1726920361161526,
              "sum": 10920,
              "p25": 198.0,
              "p50": 199.0,
              "p75": 199.0,
              "p90": 200.0,
              "p95": 200.0,
              "p99": 201.0
            },
            "input_throughput": {
              "min": 2922.2158481377614,
              "max": 9853.458560465504,
              "mean": 8878.608557136176,
              "stddev": 1231.236062716426,
              "sum": 488323.47064248973,
              "p25": 8734.816951643978,
              "p50": 9344.978822806555,
              "p75": 9618.267297880033,
              "p90": 9753.602039608633,
              "p95": 9788.380120169486,
              "p99": 9851.688148616378
            },
            "output_throughput": {
              "min": 537.2367566647039,
              "max": 657.5783645415866,
              "mean": 652.9029606870484,
              "stddev": 15.845540919907517,
              "sum": 35909.66283778766,
              "p25": 655.3343445574214,
              "p50": 655.6260433099588,
              "p75": 655.8187087497976,
              "p90": 656.151814760751,
              "p95": 656.3646671053781,
              "p99": 656.9439974905414
            }
          }
        }
      ],
      "mean_e2e_latency": 0.18958974634150655,
      "min_e2e_latency": 0.16325393100692467,
      "max_e2e_latency": 0.21592556167608842,
      "p50_e2e_latency": 0.1762570554856211,
      "p90_e2e_latency": 0.23147091530263425,
      "p99_e2e_latency": 0.30577412446029484,
      "mean_ttft": 0.02866754927998083,
      "mean_tpot": 0.0016254767379952095,
      "mean_output_throughput": 1167.432929162263,
      "max_output_throughput": 1730.0601961793327,
      "mean_total_throughput": 2316.161004683875,
      "max_total_throughput": 3431.5078583449304,
      "total_requests": 107,
      "total_completed_requests": 107,
      "total_error_requests": 0,
      "success_rate": 1.0,
      "elapsed_time": 23.6111056804657,
      "benchmark_name": "docker-simple-tune-exp2"
    },
    "container_logs": "\n==========\n== CUDA ==\n==========\n\nCUDA Version 12.6.1\n\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\nW1106 12:10:29.358000 1 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:10:29.358000 1 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n[2025-11-06 12:10:29] server_args=ServerArgs(model_path='/model', tokenizer_path='/model', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8003, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=9223372036854775807, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=478733768, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, api_key=None, served_model_name='/model', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method='round_robin', prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)\nAll deep_gemm operations loaded successfully!\n`torch_dtype` is deprecated! Use `dtype` instead!\n[2025-11-06 12:10:30] Using default HuggingFace chat template with detected content format: string\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\nW1106 12:10:35.189000 301 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:10:35.189000 301 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\nW1106 12:10:35.379000 302 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:10:35.379000 302 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n`torch_dtype` is deprecated! Use `dtype` instead!\n[2025-11-06 12:10:35] Attention backend not explicitly specified. Use fa3 backend by default.\n[2025-11-06 12:10:35] Init torch distributed begin.\n[rank0]:[W1106 12:10:35.628685980 ProcessGroupGloo.cpp:514] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[2025-11-06 12:10:36] Init torch distributed ends. mem usage=0.00 GB\n[2025-11-06 12:10:36] MOE_RUNNER_BACKEND is not initialized, using triton backend\n[2025-11-06 12:10:36] Load weight begin. avail mem=94.76 GB\nAll deep_gemm operations loaded successfully!\n\rLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]\n\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]\n\n[2025-11-06 12:10:37] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=92.35 GB, mem usage=2.41 GB.\n[2025-11-06 12:10:37] KV Cache is allocated. #tokens: 2405039, K size: 36.70 GB, V size: 36.70 GB\n[2025-11-06 12:10:37] Memory pool end. avail mem=16.82 GB\n[2025-11-06 12:10:37] Capture cuda graph begin. This can take up to several minutes. avail mem=16.72 GB\n[2025-11-06 12:10:37] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256]\n\r  0%|          | 0/35 [00:00<?, ?it/s]\rCapturing batches (bs=256 avail_mem=16.35 GB):   0%|          | 0/35 [00:00<?, ?it/s]\rCapturing batches (bs=256 avail_mem=16.35 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=248 avail_mem=16.22 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=240 avail_mem=16.22 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=232 avail_mem=16.21 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=224 avail_mem=16.21 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=224 avail_mem=16.21 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=216 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=208 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=200 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=192 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=192 avail_mem=16.20 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=184 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=176 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=168 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=160 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=160 avail_mem=16.19 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=152 avail_mem=16.18 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=144 avail_mem=16.18 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=136 avail_mem=16.18 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=128 avail_mem=16.17 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=128 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=120 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=112 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=104 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=96 avail_mem=16.16 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s] \rCapturing batches (bs=96 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=88 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=80 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=72 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=64 avail_mem=16.15 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=64 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=56 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=48 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=40 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=32 avail_mem=16.14 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=32 avail_mem=16.14 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=24 avail_mem=16.14 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=16 avail_mem=16.14 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=8 avail_mem=16.13 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s] \rCapturing batches (bs=4 avail_mem=16.13 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=4 avail_mem=16.13 GB):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  9.84it/s]\rCapturing batches (bs=2 avail_mem=16.13 GB):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  9.84it/s]\rCapturing batches (bs=1 avail_mem=16.12 GB):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  9.84it/s]\rCapturing batches (bs=1 avail_mem=16.12 GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:15<00:00,  2.19it/s]\n[2025-11-06 12:10:53] Capture cuda graph end. Time elapsed: 16.22 s. mem usage=0.60 GB. avail mem=16.12 GB.\n[2025-11-06 12:10:54] max_total_num_tokens=2405039, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=131072, available_gpu_mem=16.12 GB\n[2025-11-06 12:10:54] INFO:     Started server process [1]\n[2025-11-06 12:10:54] INFO:     Waiting for application startup.\n[2025-11-06 12:10:54] INFO:     Application startup complete.\n[2025-11-06 12:10:54] INFO:     Uvicorn running on http://0.0.0.0:8003 (Press CTRL+C to quit)\n[2025-11-06 12:10:55] INFO:     127.0.0.1:50334 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-11-06 12:10:55] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:57] INFO:     127.0.0.1:50350 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-11-06 12:10:57] The server is fired up and ready to roll!\n[2025-11-06 12:10:58] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:59] INFO:     127.0.0.1:54082 - \"GET /health HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] INFO:     127.0.0.1:54090 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 132, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 4.28, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.23, #queue-req: 0, \n[2025-11-06 12:11:03] INFO:     127.0.0.1:54094 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 541.48, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.05, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.77, #queue-req: 0, \n[2025-11-06 12:11:03] INFO:     127.0.0.1:54096 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 508.26, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.75, #queue-req: 0, \n[2025-11-06 12:11:03] INFO:     127.0.0.1:54098 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 534.80, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.35, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.82, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54114 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 560.87, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.75, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54118 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.06, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.46, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 652.29, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54122 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 38, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 534.31, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.26, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54134 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 537.16, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.38, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.16, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54144 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 540.63, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.50, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54154 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.44, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.41, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.15, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54156 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.94, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.39, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54172 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.41, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.08, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.35, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54184 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.62, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.54, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54190 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.69, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.30, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.53, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54194 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.59, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.58, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54198 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.36, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.65, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.27, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54206 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.66, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.91, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54210 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 543.46, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.22, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 643.47, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54216 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 552.64, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.91, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54230 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 521.25, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.74, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 648.86, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54240 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 534.60, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.54, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54250 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 528.40, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 652.97, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 642.88, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54252 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.83, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.32, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54258 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 536.33, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.35, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.93, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54260 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 547.36, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.96, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54266 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 546.90, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.87, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.67, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54274 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.47, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.47, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54290 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.85, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.60, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.47, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:47364 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.97, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.45, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47374 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.93, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.60, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.17, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47378 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.67, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.08, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47382 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.86, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.14, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.30, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47394 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.54, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.69, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47400 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.08, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.53, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.98, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47410 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.58, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.17, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47418 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.94, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.76, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.94, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 560.48, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.01, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47436 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.42, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.54, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.81, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47440 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 545.39, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.36, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47446 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 549.87, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.12, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.57, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47458 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.93, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.82, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47472 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.31, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 659.21, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.62, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47474 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.44, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.08, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47476 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.05, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.35, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.83, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47484 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 560.44, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.17, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47492 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.54, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.34, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.10, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47500 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.89, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.15, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47502 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.41, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.90, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.87, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47510 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.35, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.31, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47516 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.68, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.05, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.14, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47532 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 32, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.35, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.90, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47548 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.10, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.29, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.48, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47560 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.23, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.64, #queue-req: 0, \n[2025-11-06 12:11:12] INFO:     127.0.0.1:47574 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:12] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.67, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.58, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.86, #queue-req: 0, \n[2025-11-06 12:11:12] INFO:     127.0.0.1:47584 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:12] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.84, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.21, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47608 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47602 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47600 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47620 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 3, #new-token: 287, #cached-token: 111, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:13] Decode batch. #running-req: 4, #token: 486, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.16, #queue-req: 0, \n[2025-11-06 12:11:13] Decode batch. #running-req: 4, #token: 646, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2525.46, #queue-req: 0, \n[2025-11-06 12:11:13] Decode batch. #running-req: 4, #token: 806, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2482.07, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47662 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47646 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47630 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47670 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 3, #new-token: 300, #cached-token: 102, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 566, token usage: 0.00, cuda graph: True, gen throughput (token/s): 761.11, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 726, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2506.15, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47694 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 32, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47688 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47682 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 277, #cached-token: 122, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 479, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1722.99, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 639, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2521.97, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 799, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2480.97, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47730 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47724 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47712 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47742 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 2, #new-token: 175, #cached-token: 91, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 559, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1598.78, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 719, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2510.73, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47774 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47770 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47754 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47780 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 291, #cached-token: 109, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 475, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1715.38, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 635, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2525.14, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 795, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2483.72, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47800 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47794 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47788 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47812 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 309, #cached-token: 94, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 554, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1687.26, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 714, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2510.39, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47834 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47822 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47850 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 299, #cached-token: 102, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 470, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1725.94, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 630, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2522.36, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 790, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2487.24, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47886 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47872 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47858 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47900 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 186, #cached-token: 79, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 89, #cached-token: 43, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 544, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1587.24, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 704, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2514.90, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47926 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 43, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47920 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47914 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47932 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 3, #new-token: 290, #cached-token: 110, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 466, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1673.98, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 626, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2523.37, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 786, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2486.53, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47954 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47944 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47938 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 96, #cached-token: 38, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47968 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 185, #cached-token: 81, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 542, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1631.46, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 702, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2513.46, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47984 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47976 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47974 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47988 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 184, #cached-token: 81, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 39, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1465.00, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2522.29, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2487.20, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:48022 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:48018 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:48002 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:48028 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 196, #cached-token: 71, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 542, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1597.14, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 702, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2514.92, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48054 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48046 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48034 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48058 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 2, #new-token: 182, #cached-token: 82, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 40, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1582.60, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2524.42, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2489.38, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48076 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48072 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48070 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48080 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 2, #new-token: 194, #cached-token: 70, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 39, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1589.85, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2515.52, #queue-req: 0, \n",
    "objective_score": 0.18958974634150655
  },
  "all_results": [
    {
      "experiment_id": 1,
      "parameters": {
        "tp-size": 1,
        "mem-fraction-static": 0.7
      },
      "status": "success",
      "metrics": {
        "num_result_files": 2,
        "concurrency_levels": [
          4,
          1
        ],
        "raw_results": [
          {
            "scenario": "D(100,100)",
            "num_concurrency": 4,
            "batch_size": 1,
            "iteration_type": "num_concurrency",
            "run_duration": 3.0046619409695268,
            "mean_output_throughput_tokens_per_s": 1730.6439466937484,
            "mean_input_throughput_tokens_per_s": 1703.6858390625573,
            "mean_total_tokens_throughput_tokens_per_s": 3434.3297857563057,
            "mean_total_chars_per_hour": 50198645.136017926,
            "requests_per_second": 17.306439466937483,
            "error_codes_frequency": {},
            "error_rate": 0,
            "num_error_requests": 0,
            "num_completed_requests": 52,
            "num_requests": 52,
            "stats": {
              "ttft": {
                "min": 0.013084071688354015,
                "max": 0.37414403911679983,
                "mean": 0.04654311566040493,
                "stddev": 0.08078497017682842,
                "sum": 2.4202420143410563,
                "p25": 0.02423379267565906,
                "p50": 0.02784398477524519,
                "p75": 0.03211991162970662,
                "p90": 0.03417583731934428,
                "p95": 0.18660197197459497,
                "p99": 0.37339320985600355
              },
              "tpot": {
                "min": 0.0016041469826090216,
                "max": 0.005181126428222415,
                "mean": 0.0017235209858196946,
                "stddev": 0.00048565216206727463,
                "sum": 0.08962309126262412,
                "p25": 0.0016235587295295314,
                "p50": 0.0016454766466837338,
                "p75": 0.0016759456290553014,
                "p90": 0.0017217874875047592,
                "p95": 0.0017373251803971903,
                "p99": 0.003433541523650145
              },
              "e2e_latency": {
                "min": 0.18317299522459507,
                "max": 0.5335671240463853,
                "mean": 0.2171716932565547,
                "stddev": 0.09082551202841765,
                "sum": 11.292928049340844,
                "p25": 0.18876402033492923,
                "p50": 0.1909886379726231,
                "p75": 0.19497943599708378,
                "p90": 0.1983042956329882,
                "p95": 0.5293510207440704,
                "p99": 0.5334625113848597
              },
              "output_latency": {
                "min": 0.15881055127829313,
                "max": 0.5129315163940191,
                "mean": 0.17062857759614977,
                "stddev": 0.04807956404466019,
                "sum": 8.872686034999788,
                "p25": 0.1607323142234236,
                "p50": 0.16290218802168965,
                "p75": 0.16591861727647483,
                "p90": 0.17045696126297116,
                "p95": 0.17199519285932183,
                "p99": 0.3399206108413644
              },
              "output_inference_speed": {
                "min": 193.0082220253962,
                "max": 623.3842726640778,
                "mean": 596.3748034388793,
                "stddev": 58.10209207265331,
                "sum": 31011.489778821728,
                "p25": 596.6823873246229,
                "p50": 607.7266521704134,
                "p75": 615.9309205916061,
                "p90": 619.264474604818,
                "p95": 621.0247847668754,
                "p99": 622.4375130135835
              },
              "num_input_tokens": {
                "min": 95,
                "max": 100,
                "mean": 98.4423076923077,
                "stddev": 1.2921874944101799,
                "sum": 5119,
                "p25": 98.0,
                "p50": 99.0,
                "p75": 99.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 100.0
              },
              "num_output_tokens": {
                "min": 100,
                "max": 100,
                "mean": 100.0,
                "stddev": 0.0,
                "sum": 5200,
                "p25": 100.0,
                "p50": 100.0,
                "p75": 100.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 100.0
              },
              "total_tokens": {
                "min": 195,
                "max": 200,
                "mean": 198.44230769230768,
                "stddev": 1.2921874944101799,
                "sum": 10319,
                "p25": 198.0,
                "p50": 199.0,
                "p75": 199.0,
                "p90": 200.0,
                "p95": 200.0,
                "p99": 200.0
              },
              "input_throughput": {
                "min": 262.96594874477177,
                "max": 7566.451969849629,
                "mean": 3726.181080784169,
                "stddev": 1393.2675399673528,
                "sum": 193761.4162007768,
                "p25": 3089.590879342097,
                "p50": 3519.2841524296787,
                "p75": 4090.262539802275,
                "p90": 5216.130917905382,
                "p95": 6356.131166609358,
                "p99": 7396.474419053367
              },
              "output_throughput": {
                "min": 193.0082220253962,
                "max": 623.3842726640778,
                "mean": 596.3748034388793,
                "stddev": 58.10209207265331,
                "sum": 31011.489778821728,
                "p25": 596.6823873246229,
                "p50": 607.7266521704134,
                "p75": 615.9309205916061,
                "p90": 619.264474604818,
                "p95": 621.0247847668754,
                "p99": 622.4375130135835
              }
            }
          },
          {
            "scenario": "D(100,100)",
            "num_concurrency": 1,
            "batch_size": 1,
            "iteration_type": "num_concurrency",
            "run_duration": 9.09364605229348,
            "mean_output_throughput_tokens_per_s": 604.8179100409194,
            "mean_input_throughput_tokens_per_s": 595.6906579439383,
            "mean_total_tokens_throughput_tokens_per_s": 1200.5085679848578,
            "mean_total_chars_per_hour": 17547500.486692384,
            "requests_per_second": 6.048179100409195,
            "error_codes_frequency": {},
            "error_rate": 0,
            "num_error_requests": 0,
            "num_completed_requests": 55,
            "num_requests": 55,
            "stats": {
              "ttft": {
                "min": 0.00986199826002121,
                "max": 0.03203315380960703,
                "mean": 0.011397979577833955,
                "stddev": 0.0031851251845365154,
                "sum": 0.6268888767808676,
                "p25": 0.010074933990836143,
                "p50": 0.010375059209764004,
                "p75": 0.011506764218211174,
                "p90": 0.01311796996742487,
                "p95": 0.014240516163408755,
                "p99": 0.024661147426813853
              },
              "tpot": {
                "min": 0.0015213768364805164,
                "max": 0.0017966196467780104,
                "mean": 0.0015338617733605309,
                "stddev": 3.639666857887329e-05,
                "sum": 0.0843623975348292,
                "p25": 0.0015261187265166128,
                "p50": 0.0015266735972178102,
                "p75": 0.0015275891014196053,
                "p90": 0.0015393041262421945,
                "p95": 0.001548783564846022,
                "p99": 0.0016674535689555638
              },
              "e2e_latency": {
                "min": 0.16099617537111044,
                "max": 0.18827948905527592,
                "mean": 0.16325029514052652,
                "stddev": 0.004654502586128819,
                "sum": 8.978766232728958,
                "p25": 0.16123872762545943,
                "p50": 0.16177667398005724,
                "p75": 0.1631929068826139,
                "p90": 0.1652607649564743,
                "p95": 0.16717464290559292,
                "p99": 0.18523927370086313
              },
              "output_latency": {
                "min": 0.15061630681157112,
                "max": 0.17786534503102303,
                "mean": 0.15185231556269255,
                "stddev": 0.0036032701893084548,
                "sum": 8.35187735594809,
                "p25": 0.15108575392514467,
                "p50": 0.15114068612456322,
                "p75": 0.15123132104054093,
                "p90": 0.15239110849797727,
                "p95": 0.15332957291975619,
                "p99": 0.1650779033266008
              },
              "output_inference_speed": {
                "min": 556.6008374634899,
                "max": 657.2993462378159,
                "mean": 652.265431321415,
                "stddev": 13.33146849431965,
                "sum": 35874.59872267782,
                "p25": 654.6262990964076,
                "p50": 655.0188604966947,
                "p75": 655.2570143300036,
                "p90": 655.4330490816517,
                "p95": 655.5999342636811,
                "p99": 656.5254018066297
              },
              "num_input_tokens": {
                "min": 96,
                "max": 101,
                "mean": 98.49090909090908,
                "stddev": 1.2041251410762503,
                "sum": 5417,
                "p25": 98.0,
                "p50": 99.0,
                "p75": 99.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 101.0
              },
              "num_output_tokens": {
                "min": 100,
                "max": 100,
                "mean": 100.0,
                "stddev": 0.0,
                "sum": 5500,
                "p25": 100.0,
                "p50": 100.0,
                "p75": 100.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 100.0
              },
              "total_tokens": {
                "min": 196,
                "max": 201,
                "mean": 198.4909090909091,
                "stddev": 1.2041251410762506,
                "sum": 10917,
                "p25": 198.0,
                "p50": 199.0,
                "p75": 199.0,
                "p90": 200.0,
                "p95": 200.0,
                "p99": 201.0
              },
              "input_throughput": {
                "min": 3090.5480174827185,
                "max": 10038.533509109247,
                "mean": 8964.780462913168,
                "stddev": 1266.6055178382114,
                "sum": 493062.9254602242,
                "p25": 8705.722332282734,
                "p50": 9470.0484237675,
                "p75": 9779.248891497227,
                "p90": 9902.30548898911,
                "p95": 9924.272074017463,
                "p99": 10019.760589526097
              },
              "output_throughput": {
                "min": 556.6008374634899,
                "max": 657.2993462378159,
                "mean": 652.265431321415,
                "stddev": 13.331468494319646,
                "sum": 35874.59872267782,
                "p25": 654.6262990964076,
                "p50": 655.0188604966947,
                "p75": 655.2570143300036,
                "p90": 655.4330490816518,
                "p95": 655.599934263681,
                "p99": 656.5254018066297
              }
            }
          }
        ],
        "mean_e2e_latency": 0.1902109941985406,
        "min_e2e_latency": 0.16325029514052652,
        "max_e2e_latency": 0.2171716932565547,
        "p50_e2e_latency": 0.17638265597634017,
        "p90_e2e_latency": 0.18178253029473124,
        "p99_e2e_latency": 0.3593508925428614,
        "mean_ttft": 0.028970547619119442,
        "mean_tpot": 0.0016286913795901126,
        "mean_output_throughput": 1167.730928367334,
        "max_output_throughput": 1730.6439466937484,
        "mean_total_throughput": 2317.419176870582,
        "max_total_throughput": 3434.3297857563057,
        "total_requests": 107,
        "total_completed_requests": 107,
        "total_error_requests": 0,
        "success_rate": 1.0,
        "elapsed_time": 23.230535984039307,
        "benchmark_name": "docker-simple-tune-exp1"
      },
      "container_logs": "\n==========\n== CUDA ==\n==========\n\nCUDA Version 12.6.1\n\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\nW1106 12:09:25.143000 1 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:09:25.143000 1 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n[2025-11-06 12:09:25] server_args=ServerArgs(model_path='/model', tokenizer_path='/model', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8002, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.7, max_running_requests=None, max_queued_requests=9223372036854775807, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=504840253, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, api_key=None, served_model_name='/model', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method='round_robin', prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)\nAll deep_gemm operations loaded successfully!\n`torch_dtype` is deprecated! Use `dtype` instead!\n[2025-11-06 12:09:25] Using default HuggingFace chat template with detected content format: string\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\nW1106 12:09:31.069000 303 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:09:31.069000 303 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\nW1106 12:09:31.076000 302 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:09:31.076000 302 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n`torch_dtype` is deprecated! Use `dtype` instead!\n[2025-11-06 12:09:31] Attention backend not explicitly specified. Use fa3 backend by default.\n[2025-11-06 12:09:31] Init torch distributed begin.\n[rank0]:[W1106 12:09:31.419044563 ProcessGroupGloo.cpp:514] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[2025-11-06 12:09:31] Init torch distributed ends. mem usage=0.00 GB\n[2025-11-06 12:09:31] MOE_RUNNER_BACKEND is not initialized, using triton backend\n[2025-11-06 12:09:32] Load weight begin. avail mem=94.76 GB\nAll deep_gemm operations loaded successfully!\n\rLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]\n\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]\n\n[2025-11-06 12:09:33] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=92.35 GB, mem usage=2.41 GB.\n[2025-11-06 12:09:33] KV Cache is allocated. #tokens: 2094537, K size: 31.96 GB, V size: 31.96 GB\n[2025-11-06 12:09:33] Memory pool end. avail mem=26.32 GB\n[2025-11-06 12:09:33] Capture cuda graph begin. This can take up to several minutes. avail mem=26.23 GB\n[2025-11-06 12:09:33] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256]\n\r  0%|          | 0/35 [00:00<?, ?it/s]\rCapturing batches (bs=256 avail_mem=25.86 GB):   0%|          | 0/35 [00:00<?, ?it/s]\rCapturing batches (bs=256 avail_mem=25.86 GB):   3%|\u258e         | 1/35 [00:14<08:17, 14.62s/it]\rCapturing batches (bs=248 avail_mem=25.73 GB):   3%|\u258e         | 1/35 [00:14<08:17, 14.62s/it]\rCapturing batches (bs=240 avail_mem=25.72 GB):   3%|\u258e         | 1/35 [00:14<08:17, 14.62s/it]\rCapturing batches (bs=232 avail_mem=25.71 GB):   3%|\u258e         | 1/35 [00:14<08:17, 14.62s/it]\rCapturing batches (bs=232 avail_mem=25.71 GB):  11%|\u2588\u258f        | 4/35 [00:14<01:26,  2.79s/it]\rCapturing batches (bs=224 avail_mem=25.71 GB):  11%|\u2588\u258f        | 4/35 [00:14<01:26,  2.79s/it]\rCapturing batches (bs=216 avail_mem=25.70 GB):  11%|\u2588\u258f        | 4/35 [00:14<01:26,  2.79s/it]\rCapturing batches (bs=208 avail_mem=25.70 GB):  11%|\u2588\u258f        | 4/35 [00:14<01:26,  2.79s/it]\rCapturing batches (bs=208 avail_mem=25.70 GB):  20%|\u2588\u2588        | 7/35 [00:14<00:36,  1.31s/it]\rCapturing batches (bs=200 avail_mem=25.70 GB):  20%|\u2588\u2588        | 7/35 [00:14<00:36,  1.31s/it]\rCapturing batches (bs=192 avail_mem=25.70 GB):  20%|\u2588\u2588        | 7/35 [00:14<00:36,  1.31s/it]\rCapturing batches (bs=184 avail_mem=25.70 GB):  20%|\u2588\u2588        | 7/35 [00:14<00:36,  1.31s/it]\rCapturing batches (bs=184 avail_mem=25.70 GB):  29%|\u2588\u2588\u258a       | 10/35 [00:14<00:18,  1.32it/s]\rCapturing batches (bs=176 avail_mem=25.70 GB):  29%|\u2588\u2588\u258a       | 10/35 [00:14<00:18,  1.32it/s]\rCapturing batches (bs=168 avail_mem=25.69 GB):  29%|\u2588\u2588\u258a       | 10/35 [00:14<00:18,  1.32it/s]\rCapturing batches (bs=160 avail_mem=25.69 GB):  29%|\u2588\u2588\u258a       | 10/35 [00:15<00:18,  1.32it/s]\rCapturing batches (bs=160 avail_mem=25.69 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:10,  2.07it/s]\rCapturing batches (bs=152 avail_mem=25.69 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:10,  2.07it/s]\rCapturing batches (bs=144 avail_mem=25.68 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:10,  2.07it/s]\rCapturing batches (bs=136 avail_mem=25.68 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:10,  2.07it/s]\rCapturing batches (bs=136 avail_mem=25.68 GB):  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:15<00:06,  3.07it/s]\rCapturing batches (bs=128 avail_mem=25.68 GB):  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:15<00:06,  3.07it/s]\rCapturing batches (bs=120 avail_mem=25.68 GB):  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:15<00:06,  3.07it/s]\rCapturing batches (bs=112 avail_mem=25.67 GB):  46%|\u2588\u2588\u2588\u2588\u258c     | 16/35 [00:15<00:06,  3.07it/s]\rCapturing batches (bs=112 avail_mem=25.67 GB):  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:15<00:03,  4.40it/s]\rCapturing batches (bs=104 avail_mem=25.67 GB):  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:15<00:03,  4.40it/s]\rCapturing batches (bs=96 avail_mem=25.67 GB):  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:15<00:03,  4.40it/s] \rCapturing batches (bs=88 avail_mem=25.66 GB):  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:15<00:03,  4.40it/s]\rCapturing batches (bs=80 avail_mem=25.66 GB):  54%|\u2588\u2588\u2588\u2588\u2588\u258d    | 19/35 [00:15<00:03,  4.40it/s]\rCapturing batches (bs=80 avail_mem=25.66 GB):  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:15<00:01,  6.62it/s]\rCapturing batches (bs=72 avail_mem=25.66 GB):  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:15<00:01,  6.62it/s]\rCapturing batches (bs=64 avail_mem=25.66 GB):  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:15<00:01,  6.62it/s]\rCapturing batches (bs=56 avail_mem=25.65 GB):  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:15<00:01,  6.62it/s]\rCapturing batches (bs=48 avail_mem=25.65 GB):  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u258c   | 23/35 [00:15<00:01,  6.62it/s]\rCapturing batches (bs=48 avail_mem=25.65 GB):  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:15<00:00,  9.28it/s]\rCapturing batches (bs=40 avail_mem=25.65 GB):  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:15<00:00,  9.28it/s]\rCapturing batches (bs=32 avail_mem=25.65 GB):  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:15<00:00,  9.28it/s]\rCapturing batches (bs=24 avail_mem=25.64 GB):  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:15<00:00,  9.28it/s]\rCapturing batches (bs=16 avail_mem=25.64 GB):  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b  | 27/35 [00:15<00:00,  9.28it/s]\rCapturing batches (bs=16 avail_mem=25.64 GB):  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:00, 11.79it/s]\rCapturing batches (bs=8 avail_mem=25.64 GB):  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:00, 11.79it/s] \rCapturing batches (bs=4 avail_mem=25.63 GB):  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:00, 11.79it/s]\rCapturing batches (bs=2 avail_mem=25.63 GB):  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:00, 11.79it/s]\rCapturing batches (bs=1 avail_mem=25.63 GB):  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258a | 31/35 [00:15<00:00, 11.79it/s]\rCapturing batches (bs=1 avail_mem=25.63 GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:15<00:00,  2.22it/s]\n[2025-11-06 12:09:49] Capture cuda graph end. Time elapsed: 15.99 s. mem usage=0.60 GB. avail mem=25.62 GB.\n[2025-11-06 12:09:49] max_total_num_tokens=2094537, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=131072, available_gpu_mem=25.62 GB\n[2025-11-06 12:09:50] INFO:     Started server process [1]\n[2025-11-06 12:09:50] INFO:     Waiting for application startup.\n[2025-11-06 12:09:50] INFO:     Application startup complete.\n[2025-11-06 12:09:50] INFO:     Uvicorn running on http://0.0.0.0:8002 (Press CTRL+C to quit)\n[2025-11-06 12:09:51] INFO:     127.0.0.1:59024 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-11-06 12:09:51] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:53] INFO:     127.0.0.1:59036 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-11-06 12:09:53] The server is fired up and ready to roll!\n[2025-11-06 12:09:54] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:55] INFO:     127.0.0.1:59052 - \"GET /health HTTP/1.1\" 200 OK\n[2025-11-06 12:09:58] INFO:     127.0.0.1:51620 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:58] Prefill batch. #new-seq: 1, #new-token: 133, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:58] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 4.41, #queue-req: 0, \n[2025-11-06 12:09:58] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.70, #queue-req: 0, \n[2025-11-06 12:09:58] INFO:     127.0.0.1:51636 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:58] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:58] Decode batch. #running-req: 1, #token: 143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 493.18, #queue-req: 0, \n[2025-11-06 12:09:58] Decode batch. #running-req: 1, #token: 183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.98, #queue-req: 0, \n[2025-11-06 12:09:58] Decode batch. #running-req: 1, #token: 223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.60, #queue-req: 0, \n[2025-11-06 12:09:58] INFO:     127.0.0.1:51646 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:58] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 163, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.31, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 203, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.75, #queue-req: 0, \n[2025-11-06 12:09:59] INFO:     127.0.0.1:51658 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:59] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 533.26, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.84, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.04, #queue-req: 0, \n[2025-11-06 12:09:59] INFO:     127.0.0.1:51660 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:59] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 537.24, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.41, #queue-req: 0, \n[2025-11-06 12:09:59] INFO:     127.0.0.1:51666 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:59] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.45, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.42, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.73, #queue-req: 0, \n[2025-11-06 12:09:59] INFO:     127.0.0.1:51676 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:59] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 546.90, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.20, #queue-req: 0, \n[2025-11-06 12:09:59] INFO:     127.0.0.1:51690 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:59] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 548.78, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.89, #queue-req: 0, \n[2025-11-06 12:09:59] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.55, #queue-req: 0, \n[2025-11-06 12:09:59] INFO:     127.0.0.1:51698 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:09:59] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 527.64, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.04, #queue-req: 0, \n[2025-11-06 12:10:00] INFO:     127.0.0.1:51714 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:00] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 551.81, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.63, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.98, #queue-req: 0, \n[2025-11-06 12:10:00] INFO:     127.0.0.1:51718 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:00] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.00, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.79, #queue-req: 0, \n[2025-11-06 12:10:00] INFO:     127.0.0.1:51730 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:00] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.08, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.06, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.46, #queue-req: 0, \n[2025-11-06 12:10:00] INFO:     127.0.0.1:51734 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:00] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 163, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.70, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 203, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.25, #queue-req: 0, \n[2025-11-06 12:10:00] INFO:     127.0.0.1:51746 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:00] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 547.20, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.20, #queue-req: 0, \n[2025-11-06 12:10:00] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.37, #queue-req: 0, \n[2025-11-06 12:10:00] INFO:     127.0.0.1:51752 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:00] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.70, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.32, #queue-req: 0, \n[2025-11-06 12:10:01] INFO:     127.0.0.1:51766 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:01] Prefill batch. #new-seq: 1, #new-token: 96, #cached-token: 38, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.87, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.74, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.58, #queue-req: 0, \n[2025-11-06 12:10:01] INFO:     127.0.0.1:51776 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:01] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 560.64, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.82, #queue-req: 0, \n[2025-11-06 12:10:01] INFO:     127.0.0.1:51780 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:01] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.49, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.31, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.63, #queue-req: 0, \n[2025-11-06 12:10:01] INFO:     127.0.0.1:51796 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:01] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.15, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.38, #queue-req: 0, \n[2025-11-06 12:10:01] INFO:     127.0.0.1:51802 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:01] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.24, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.75, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.73, #queue-req: 0, \n[2025-11-06 12:10:01] INFO:     127.0.0.1:51810 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:01] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:01] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.39, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.63, #queue-req: 0, \n[2025-11-06 12:10:02] INFO:     127.0.0.1:51818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:02] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.06, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 659.01, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.84, #queue-req: 0, \n[2025-11-06 12:10:02] INFO:     127.0.0.1:51832 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:02] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 562.80, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.96, #queue-req: 0, \n[2025-11-06 12:10:02] INFO:     127.0.0.1:51836 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:02] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.48, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.91, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.68, #queue-req: 0, \n[2025-11-06 12:10:02] INFO:     127.0.0.1:51846 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:02] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 562.48, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.95, #queue-req: 0, \n[2025-11-06 12:10:02] INFO:     127.0.0.1:51856 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:02] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.05, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.31, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.57, #queue-req: 0, \n[2025-11-06 12:10:02] INFO:     127.0.0.1:51868 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:02] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:02] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 562.11, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.33, #queue-req: 0, \n[2025-11-06 12:10:03] INFO:     127.0.0.1:51876 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:03] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.85, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.08, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.67, #queue-req: 0, \n[2025-11-06 12:10:03] INFO:     127.0.0.1:51888 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:03] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.75, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.34, #queue-req: 0, \n[2025-11-06 12:10:03] INFO:     127.0.0.1:51898 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:03] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.36, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.98, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.06, #queue-req: 0, \n[2025-11-06 12:10:03] INFO:     127.0.0.1:51902 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:03] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 539.41, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.63, #queue-req: 0, \n[2025-11-06 12:10:03] INFO:     127.0.0.1:51918 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:03] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.46, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.22, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.98, #queue-req: 0, \n[2025-11-06 12:10:03] INFO:     127.0.0.1:51930 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:03] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:03] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 562.45, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.43, #queue-req: 0, \n[2025-11-06 12:10:04] INFO:     127.0.0.1:51938 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:04] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 32, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 551.42, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.82, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.00, #queue-req: 0, \n[2025-11-06 12:10:04] INFO:     127.0.0.1:51942 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:04] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 563.22, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.26, #queue-req: 0, \n[2025-11-06 12:10:04] INFO:     127.0.0.1:51944 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:04] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.94, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.31, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.91, #queue-req: 0, \n[2025-11-06 12:10:04] INFO:     127.0.0.1:51948 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:04] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.72, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.08, #queue-req: 0, \n[2025-11-06 12:10:04] INFO:     127.0.0.1:51950 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:04] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.94, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.32, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.05, #queue-req: 0, \n[2025-11-06 12:10:04] INFO:     127.0.0.1:51962 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:04] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 43, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 547.93, #queue-req: 0, \n[2025-11-06 12:10:04] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.63, #queue-req: 0, \n[2025-11-06 12:10:05] INFO:     127.0.0.1:51976 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:05] Prefill batch. #new-seq: 1, #new-token: 100, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 537.61, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.91, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.21, #queue-req: 0, \n[2025-11-06 12:10:05] INFO:     127.0.0.1:51988 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:05] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 562.85, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.79, #queue-req: 0, \n[2025-11-06 12:10:05] INFO:     127.0.0.1:52002 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:05] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.86, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.17, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.44, #queue-req: 0, \n[2025-11-06 12:10:05] INFO:     127.0.0.1:52006 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:05] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 38, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 562.54, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.88, #queue-req: 0, \n[2025-11-06 12:10:05] INFO:     127.0.0.1:52010 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:05] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.05, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.28, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.68, #queue-req: 0, \n[2025-11-06 12:10:05] INFO:     127.0.0.1:52022 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:05] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.21, #queue-req: 0, \n[2025-11-06 12:10:05] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 645.36, #queue-req: 0, \n[2025-11-06 12:10:06] INFO:     127.0.0.1:52028 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:06] Prefill batch. #new-seq: 1, #new-token: 97, #cached-token: 37, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 520.37, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.18, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 648.26, #queue-req: 0, \n[2025-11-06 12:10:06] INFO:     127.0.0.1:52030 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:06] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 545.22, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.54, #queue-req: 0, \n[2025-11-06 12:10:06] INFO:     127.0.0.1:52034 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:06] Prefill batch. #new-seq: 1, #new-token: 96, #cached-token: 38, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 534.31, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.90, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.22, #queue-req: 0, \n[2025-11-06 12:10:06] INFO:     127.0.0.1:52044 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:06] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 545.94, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 646.84, #queue-req: 0, \n[2025-11-06 12:10:06] INFO:     127.0.0.1:52060 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:06] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 532.86, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 648.35, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 633.36, #queue-req: 0, \n[2025-11-06 12:10:06] INFO:     127.0.0.1:52064 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:06] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 43, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 554.58, #queue-req: 0, \n[2025-11-06 12:10:06] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.07, #queue-req: 0, \n[2025-11-06 12:10:07] INFO:     127.0.0.1:52070 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:07] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 544.95, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.05, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 645.54, #queue-req: 0, \n[2025-11-06 12:10:07] INFO:     127.0.0.1:52080 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:07] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 546.52, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.58, #queue-req: 0, \n[2025-11-06 12:10:07] INFO:     127.0.0.1:52086 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:07] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.80, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.44, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.18, #queue-req: 0, \n[2025-11-06 12:10:07] INFO:     127.0.0.1:52090 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:07] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.84, #queue-req: 0, \n[2025-11-06 12:10:07] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.14, #queue-req: 0, \n[2025-11-06 12:10:08] INFO:     127.0.0.1:57354 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:08] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:08] INFO:     127.0.0.1:57342 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:08] INFO:     127.0.0.1:57338 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:08] INFO:     127.0.0.1:57368 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:08] Prefill batch. #new-seq: 3, #new-token: 309, #cached-token: 93, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 491, token usage: 0.00, cuda graph: True, gen throughput (token/s): 47.59, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 651, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2522.49, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 811, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2477.37, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57402 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57386 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57380 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57418 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 3, #new-token: 286, #cached-token: 114, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 565, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1593.36, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 725, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2506.15, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57446 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57442 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57452 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 2, #new-token: 197, #cached-token: 70, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:09] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 41, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 481, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1614.65, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 641, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2519.32, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 801, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2484.42, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57468 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 1, #new-token: 81, #cached-token: 54, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57458 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57456 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57478 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 3, #new-token: 281, #cached-token: 119, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 562, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1625.56, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 722, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2505.03, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57504 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57492 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] INFO:     127.0.0.1:57488 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:09] INFO:     127.0.0.1:57506 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:09] Prefill batch. #new-seq: 2, #new-token: 193, #cached-token: 73, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:09] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 31, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 482, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1631.91, #queue-req: 0, \n[2025-11-06 12:10:09] Decode batch. #running-req: 4, #token: 642, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2521.91, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 802, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2482.82, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57542 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57536 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57522 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 89, #cached-token: 44, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57546 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 2, #new-token: 206, #cached-token: 61, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 97, #cached-token: 38, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 561, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1590.12, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 721, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2507.35, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57568 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57556 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57554 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57572 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 2, #new-token: 197, #cached-token: 70, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 39, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 480, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1633.98, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 640, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2520.79, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 800, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2483.98, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57596 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57592 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57582 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57604 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 2, #new-token: 178, #cached-token: 83, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 41, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 555, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1598.38, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 715, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2506.41, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57612 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57608 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57606 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:10] Prefill batch. #new-seq: 2, #new-token: 206, #cached-token: 61, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57616 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 41, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 481, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1625.16, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 641, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2522.47, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 801, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2479.85, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57634 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:10] INFO:     127.0.0.1:57624 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57620 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] INFO:     127.0.0.1:57644 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:10] Prefill batch. #new-seq: 3, #new-token: 301, #cached-token: 101, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 556, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1642.91, #queue-req: 0, \n[2025-11-06 12:10:10] Decode batch. #running-req: 4, #token: 716, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2508.09, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57666 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57662 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57658 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57676 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 2, #new-token: 189, #cached-token: 77, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 39, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 472, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1635.26, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 632, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2520.24, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 792, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2484.93, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57702 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57688 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57716 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 2, #new-token: 196, #cached-token: 71, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 96, #cached-token: 39, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 556, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1600.41, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 716, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2506.17, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57730 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57724 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57720 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57744 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 2, #new-token: 187, #cached-token: 78, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 476, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1608.02, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 636, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2523.02, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 796, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2481.53, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57766 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57756 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] INFO:     127.0.0.1:57746 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 97, #cached-token: 37, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:11] INFO:     127.0.0.1:57770 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:10:11] Prefill batch. #new-seq: 2, #new-token: 190, #cached-token: 75, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:10:11] Prefill batch. #new-seq: 1, #new-token: 89, #cached-token: 42, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 552, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1614.07, #queue-req: 0, \n[2025-11-06 12:10:11] Decode batch. #running-req: 4, #token: 712, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2511.08, #queue-req: 0, \n",
      "objective_score": 0.1902109941985406
    },
    {
      "experiment_id": 2,
      "parameters": {
        "tp-size": 1,
        "mem-fraction-static": 0.8
      },
      "status": "success",
      "metrics": {
        "num_result_files": 2,
        "concurrency_levels": [
          4,
          1
        ],
        "raw_results": [
          {
            "scenario": "D(100,100)",
            "num_concurrency": 4,
            "batch_size": 1,
            "iteration_type": "num_concurrency",
            "run_duration": 3.0056757628917694,
            "mean_output_throughput_tokens_per_s": 1730.0601961793327,
            "mean_input_throughput_tokens_per_s": 1701.4476621655976,
            "mean_total_tokens_throughput_tokens_per_s": 3431.5078583449304,
            "mean_total_chars_per_hour": 50157397.806390256,
            "requests_per_second": 17.300601961793326,
            "error_codes_frequency": {},
            "error_rate": 0,
            "num_error_requests": 0,
            "num_completed_requests": 52,
            "num_requests": 52,
            "stats": {
              "ttft": {
                "min": 0.011742539703845978,
                "max": 0.26349610928446054,
                "mean": 0.04581959448898068,
                "stddev": 0.06004420390343907,
                "sum": 2.3826189134269953,
                "p25": 0.021264498122036457,
                "p50": 0.027980441693216562,
                "p75": 0.03369458741508424,
                "p90": 0.12693287534639225,
                "p95": 0.19588832021690855,
                "p99": 0.26273496785201134
              },
              "tpot": {
                "min": 0.0016091428697109222,
                "max": 0.004043731010622448,
                "mean": 0.0017182420927990686,
                "stddev": 0.00036644888163548094,
                "sum": 0.08934858882555156,
                "p25": 0.0016215999300281207,
                "p50": 0.001634153565674117,
                "p75": 0.0016712439575731152,
                "p90": 0.0017361357723447409,
                "p95": 0.0017693769774691323,
                "p99": 0.0034281404273151773
              },
              "e2e_latency": {
                "min": 0.1785618867725134,
                "max": 0.42280125338584185,
                "mean": 0.21592556167608842,
                "stddev": 0.06610715847078844,
                "sum": 11.228129207156599,
                "p25": 0.18564055324532092,
                "p50": 0.1909139105118811,
                "p75": 0.1995104046072811,
                "p90": 0.298194214515388,
                "p95": 0.41844711694866416,
                "p99": 0.4226723491027951
              },
              "output_latency": {
                "min": 0.1593051441013813,
                "max": 0.4003293700516224,
                "mean": 0.17010596718710774,
                "stddev": 0.03627843928191263,
                "sum": 8.845510293729603,
                "p25": 0.16053839307278395,
                "p50": 0.1617812030017376,
                "p75": 0.1654531517997384,
                "p90": 0.17187744146212935,
                "p95": 0.17516832076944408,
                "p99": 0.33938590230420257
              },
              "output_inference_speed": {
                "min": 247.296369954655,
                "max": 621.4488587825934,
                "mean": 594.8696381599027,
                "stddev": 61.61944967991773,
                "sum": 30933.221184314938,
                "p25": 598.3646504684243,
                "p50": 611.9375966246494,
                "p75": 616.6749185776496,
                "p90": 620.6711587244066,
                "p95": 621.1654001162117,
                "p99": 621.4126095951355
              },
              "num_input_tokens": {
                "min": 95,
                "max": 100,
                "mean": 98.34615384615384,
                "stddev": 1.1748095653308666,
                "sum": 5114,
                "p25": 97.75,
                "p50": 98.0,
                "p75": 99.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 100.0
              },
              "num_output_tokens": {
                "min": 100,
                "max": 100,
                "mean": 100.0,
                "stddev": 0.0,
                "sum": 5200,
                "p25": 100.0,
                "p50": 100.0,
                "p75": 100.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 100.0
              },
              "total_tokens": {
                "min": 195,
                "max": 200,
                "mean": 198.34615384615384,
                "stddev": 1.1748095653308666,
                "sum": 10314,
                "p25": 197.75,
                "p50": 198.0,
                "p75": 199.0,
                "p90": 200.0,
                "p95": 200.0,
                "p99": 200.0
              },
              "input_throughput": {
                "min": 364.33175526080345,
                "max": 8345.724389409774,
                "mean": 3846.5724401047637,
                "stddev": 1940.820954921548,
                "sum": 200021.76688544772,
                "p25": 2943.5514298465278,
                "p50": 3517.718045740773,
                "p75": 4585.774827205975,
                "p90": 7376.0324599542555,
                "p95": 8048.303684886858,
                "p99": 8299.675796735595
              },
              "output_throughput": {
                "min": 247.296369954655,
                "max": 621.4488587825934,
                "mean": 594.8696381599027,
                "stddev": 61.61944967991773,
                "sum": 30933.221184314938,
                "p25": 598.3646504684243,
                "p50": 611.9375966246494,
                "p75": 616.6749185776497,
                "p90": 620.6711587244065,
                "p95": 621.1654001162117,
                "p99": 621.4126095951355
              }
            }
          },
          {
            "scenario": "D(100,100)",
            "num_concurrency": 1,
            "batch_size": 1,
            "iteration_type": "num_concurrency",
            "run_duration": 9.093830207362771,
            "mean_output_throughput_tokens_per_s": 604.8056621451932,
            "mean_input_throughput_tokens_per_s": 596.0084888776267,
            "mean_total_tokens_throughput_tokens_per_s": 1200.81415102282,
            "mean_total_chars_per_hour": 17551967.10913088,
            "requests_per_second": 6.048056621451932,
            "error_codes_frequency": {},
            "error_rate": 0,
            "num_error_requests": 0,
            "num_completed_requests": 55,
            "num_requests": 55,
            "stats": {
              "ttft": {
                "min": 0.010148720815777779,
                "max": 0.03353619482368231,
                "mean": 0.011515504070980983,
                "stddev": 0.0033043659980104877,
                "sum": 0.633352723903954,
                "p25": 0.010281766764819622,
                "p50": 0.010465477593243122,
                "p75": 0.01126451464369893,
                "p90": 0.013171770796179772,
                "p95": 0.014313541539013383,
                "p99": 0.024850562494248166
              },
              "tpot": {
                "min": 0.0015207312982341255,
                "max": 0.0018613767349208244,
                "mean": 0.0015327113831913504,
                "stddev": 4.493010490665829e-05,
                "sum": 0.08429912607552427,
                "p25": 0.0015248116388013868,
                "p50": 0.0015252597272546605,
                "p75": 0.0015259386425969575,
                "p90": 0.001533680691412001,
                "p95": 0.001537989540909878,
                "p99": 0.0016904403570324486
              },
              "e2e_latency": {
                "min": 0.16107304487377405,
                "max": 0.19449578132480383,
                "mean": 0.16325393100692467,
                "stddev": 0.005373834061503619,
                "sum": 8.978966205380857,
                "p25": 0.16131580481305718,
                "p50": 0.16160020045936108,
                "p75": 0.16267037438228726,
                "p90": 0.16474761608988048,
                "p95": 0.16699957717210054,
                "p99": 0.18887589981779457
              },
              "output_latency": {
                "min": 0.15055239852517843,
                "max": 0.18427629675716162,
                "mean": 0.1517384269359437,
                "stddev": 0.004448080385759171,
                "sum": 8.345613481476903,
                "p25": 0.1509563522413373,
                "p50": 0.15100071299821138,
                "p75": 0.1510679256170988,
                "p90": 0.15183438844978808,
                "p95": 0.15226096455007793,
                "p99": 0.1673535953462124
              },
              "output_inference_speed": {
                "min": 537.2367566647039,
                "max": 657.5783645415866,
                "mean": 652.9029606870484,
                "stddev": 15.845540919907515,
                "sum": 35909.66283778766,
                "p25": 655.3343445574214,
                "p50": 655.6260433099588,
                "p75": 655.8187087497976,
                "p90": 656.151814760751,
                "p95": 656.3646671053781,
                "p99": 656.9439974905414
              },
              "num_input_tokens": {
                "min": 96,
                "max": 101,
                "mean": 98.54545454545455,
                "stddev": 1.1726920361161526,
                "sum": 5420,
                "p25": 98.0,
                "p50": 99.0,
                "p75": 99.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 101.0
              },
              "num_output_tokens": {
                "min": 100,
                "max": 100,
                "mean": 100.0,
                "stddev": 0.0,
                "sum": 5500,
                "p25": 100.0,
                "p50": 100.0,
                "p75": 100.0,
                "p90": 100.0,
                "p95": 100.0,
                "p99": 100.0
              },
              "total_tokens": {
                "min": 196,
                "max": 201,
                "mean": 198.54545454545453,
                "stddev": 1.1726920361161526,
                "sum": 10920,
                "p25": 198.0,
                "p50": 199.0,
                "p75": 199.0,
                "p90": 200.0,
                "p95": 200.0,
                "p99": 201.0
              },
              "input_throughput": {
                "min": 2922.2158481377614,
                "max": 9853.458560465504,
                "mean": 8878.608557136176,
                "stddev": 1231.236062716426,
                "sum": 488323.47064248973,
                "p25": 8734.816951643978,
                "p50": 9344.978822806555,
                "p75": 9618.267297880033,
                "p90": 9753.602039608633,
                "p95": 9788.380120169486,
                "p99": 9851.688148616378
              },
              "output_throughput": {
                "min": 537.2367566647039,
                "max": 657.5783645415866,
                "mean": 652.9029606870484,
                "stddev": 15.845540919907517,
                "sum": 35909.66283778766,
                "p25": 655.3343445574214,
                "p50": 655.6260433099588,
                "p75": 655.8187087497976,
                "p90": 656.151814760751,
                "p95": 656.3646671053781,
                "p99": 656.9439974905414
              }
            }
          }
        ],
        "mean_e2e_latency": 0.18958974634150655,
        "min_e2e_latency": 0.16325393100692467,
        "max_e2e_latency": 0.21592556167608842,
        "p50_e2e_latency": 0.1762570554856211,
        "p90_e2e_latency": 0.23147091530263425,
        "p99_e2e_latency": 0.30577412446029484,
        "mean_ttft": 0.02866754927998083,
        "mean_tpot": 0.0016254767379952095,
        "mean_output_throughput": 1167.432929162263,
        "max_output_throughput": 1730.0601961793327,
        "mean_total_throughput": 2316.161004683875,
        "max_total_throughput": 3431.5078583449304,
        "total_requests": 107,
        "total_completed_requests": 107,
        "total_error_requests": 0,
        "success_rate": 1.0,
        "elapsed_time": 23.6111056804657,
        "benchmark_name": "docker-simple-tune-exp2"
      },
      "container_logs": "\n==========\n== CUDA ==\n==========\n\nCUDA Version 12.6.1\n\nContainer image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n\nThis container image and its contents are governed by the NVIDIA Deep Learning Container License.\nBy pulling and using the container, you accept the terms and conditions of this license:\nhttps://developer.nvidia.com/ngc/nvidia-deep-learning-container-license\n\nA copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.\n\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\nW1106 12:10:29.358000 1 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:10:29.358000 1 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n[2025-11-06 12:10:29] server_args=ServerArgs(model_path='/model', tokenizer_path='/model', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, revision=None, model_impl='auto', host='0.0.0.0', port=8003, skip_server_warmup=False, warmups=None, nccl_port=None, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', mem_fraction_static=0.8, max_running_requests=None, max_queued_requests=9223372036854775807, max_total_tokens=None, chunked_prefill_size=8192, max_prefill_tokens=16384, schedule_policy='fcfs', schedule_conservativeness=1.0, page_size=1, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, device='cuda', tp_size=1, pp_size=1, max_micro_batch_size=None, stream_interval=1, stream_output=False, random_seed=478733768, constrained_json_whitespace_pattern=None, watchdog_timeout=300, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, gc_warning_threshold_secs=0.0, api_key=None, served_model_name='/model', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, dp_size=1, load_balance_method='round_robin', prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_backend='triton', attention_backend=None, decode_attention_backend=None, prefill_attention_backend=None, sampling_backend='flashinfer', grammar_backend='xgrammar', mm_attention_backend=None, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm='static', init_expert_location='trivial', enable_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', disable_radix_cache=False, cuda_graph_max_bs=None, cuda_graph_bs=None, disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=False, enable_mscclpp=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, torch_compile_max_bs=32, torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, enable_return_hidden_states=False, scheduler_recv_interval=1, numa_node=None, debug_tensor_dump_output_folder=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, debug_tensor_dump_prefill_only=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, num_reserved_decode_tokens=512, custom_weight_loader=[], weight_loader_disable_mmap=False, enable_pdmux=False, sm_group_num=3, max_mamba_cache_size=None, mamba_ssm_dtype='float32', enable_ep_moe=False, enable_deepep_moe=False, enable_flashinfer_cutlass_moe=False, enable_flashinfer_trtllm_moe=False, enable_triton_kernel_moe=False, enable_flashinfer_mxfp4_moe=False)\nAll deep_gemm operations loaded successfully!\n`torch_dtype` is deprecated! Use `dtype` instead!\n[2025-11-06 12:10:30] Using default HuggingFace chat template with detected content format: string\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\n/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py:63: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n  import pynvml  # type: ignore[import]\nW1106 12:10:35.189000 301 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:10:35.189000 301 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\nW1106 12:10:35.379000 302 torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \nW1106 12:10:35.379000 302 torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.\n`torch_dtype` is deprecated! Use `dtype` instead!\n[2025-11-06 12:10:35] Attention backend not explicitly specified. Use fa3 backend by default.\n[2025-11-06 12:10:35] Init torch distributed begin.\n[rank0]:[W1106 12:10:35.628685980 ProcessGroupGloo.cpp:514] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n[2025-11-06 12:10:36] Init torch distributed ends. mem usage=0.00 GB\n[2025-11-06 12:10:36] MOE_RUNNER_BACKEND is not initialized, using triton backend\n[2025-11-06 12:10:36] Load weight begin. avail mem=94.76 GB\nAll deep_gemm operations loaded successfully!\n\rLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]\n\rLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.89it/s]\n\n[2025-11-06 12:10:37] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=92.35 GB, mem usage=2.41 GB.\n[2025-11-06 12:10:37] KV Cache is allocated. #tokens: 2405039, K size: 36.70 GB, V size: 36.70 GB\n[2025-11-06 12:10:37] Memory pool end. avail mem=16.82 GB\n[2025-11-06 12:10:37] Capture cuda graph begin. This can take up to several minutes. avail mem=16.72 GB\n[2025-11-06 12:10:37] Capture cuda graph bs [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256]\n\r  0%|          | 0/35 [00:00<?, ?it/s]\rCapturing batches (bs=256 avail_mem=16.35 GB):   0%|          | 0/35 [00:00<?, ?it/s]\rCapturing batches (bs=256 avail_mem=16.35 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=248 avail_mem=16.22 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=240 avail_mem=16.22 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=232 avail_mem=16.21 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=224 avail_mem=16.21 GB):   3%|\u258e         | 1/35 [00:14<08:26, 14.91s/it]\rCapturing batches (bs=224 avail_mem=16.21 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=216 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=208 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=200 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=192 avail_mem=16.20 GB):  14%|\u2588\u258d        | 5/35 [00:15<01:07,  2.25s/it]\rCapturing batches (bs=192 avail_mem=16.20 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=184 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=176 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=168 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=160 avail_mem=16.19 GB):  26%|\u2588\u2588\u258c       | 9/35 [00:15<00:26,  1.03s/it]\rCapturing batches (bs=160 avail_mem=16.19 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=152 avail_mem=16.18 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=144 avail_mem=16.18 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=136 avail_mem=16.18 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=128 avail_mem=16.17 GB):  37%|\u2588\u2588\u2588\u258b      | 13/35 [00:15<00:13,  1.69it/s]\rCapturing batches (bs=128 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=120 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=112 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=104 avail_mem=16.17 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s]\rCapturing batches (bs=96 avail_mem=16.16 GB):  49%|\u2588\u2588\u2588\u2588\u258a     | 17/35 [00:15<00:06,  2.65it/s] \rCapturing batches (bs=96 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=88 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=80 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=72 avail_mem=16.16 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=64 avail_mem=16.15 GB):  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 21/35 [00:15<00:03,  3.93it/s]\rCapturing batches (bs=64 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=56 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=48 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=40 avail_mem=16.15 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=32 avail_mem=16.14 GB):  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 25/35 [00:15<00:01,  5.57it/s]\rCapturing batches (bs=32 avail_mem=16.14 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=24 avail_mem=16.14 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=16 avail_mem=16.14 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=8 avail_mem=16.13 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s] \rCapturing batches (bs=4 avail_mem=16.13 GB):  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e | 29/35 [00:15<00:00,  7.64it/s]\rCapturing batches (bs=4 avail_mem=16.13 GB):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  9.84it/s]\rCapturing batches (bs=2 avail_mem=16.13 GB):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  9.84it/s]\rCapturing batches (bs=1 avail_mem=16.12 GB):  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d| 33/35 [00:15<00:00,  9.84it/s]\rCapturing batches (bs=1 avail_mem=16.12 GB): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 35/35 [00:15<00:00,  2.19it/s]\n[2025-11-06 12:10:53] Capture cuda graph end. Time elapsed: 16.22 s. mem usage=0.60 GB. avail mem=16.12 GB.\n[2025-11-06 12:10:54] max_total_num_tokens=2405039, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=131072, available_gpu_mem=16.12 GB\n[2025-11-06 12:10:54] INFO:     Started server process [1]\n[2025-11-06 12:10:54] INFO:     Waiting for application startup.\n[2025-11-06 12:10:54] INFO:     Application startup complete.\n[2025-11-06 12:10:54] INFO:     Uvicorn running on http://0.0.0.0:8003 (Press CTRL+C to quit)\n[2025-11-06 12:10:55] INFO:     127.0.0.1:50334 - \"GET /get_model_info HTTP/1.1\" 200 OK\n[2025-11-06 12:10:55] Prefill batch. #new-seq: 1, #new-token: 7, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:57] INFO:     127.0.0.1:50350 - \"POST /generate HTTP/1.1\" 200 OK\n[2025-11-06 12:10:57] The server is fired up and ready to roll!\n[2025-11-06 12:10:58] Prefill batch. #new-seq: 1, #new-token: 1, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:10:59] INFO:     127.0.0.1:54082 - \"GET /health HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] INFO:     127.0.0.1:54090 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 132, #cached-token: 1, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 4.28, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.23, #queue-req: 0, \n[2025-11-06 12:11:03] INFO:     127.0.0.1:54094 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 541.48, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.05, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.77, #queue-req: 0, \n[2025-11-06 12:11:03] INFO:     127.0.0.1:54096 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 508.26, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.75, #queue-req: 0, \n[2025-11-06 12:11:03] INFO:     127.0.0.1:54098 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:03] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 534.80, #queue-req: 0, \n[2025-11-06 12:11:03] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.35, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.82, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54114 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 560.87, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.75, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54118 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.06, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.46, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 652.29, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54122 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 38, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 534.31, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.26, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54134 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 537.16, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.38, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.16, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54144 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 540.63, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.50, #queue-req: 0, \n[2025-11-06 12:11:04] INFO:     127.0.0.1:54154 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:04] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.44, #queue-req: 0, \n[2025-11-06 12:11:04] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.41, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.15, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54156 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.94, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.39, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54172 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.41, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.08, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.35, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54184 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.62, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.54, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54190 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.69, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.30, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.53, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54194 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.59, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.58, #queue-req: 0, \n[2025-11-06 12:11:05] INFO:     127.0.0.1:54198 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:05] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.36, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.65, #queue-req: 0, \n[2025-11-06 12:11:05] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.27, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54206 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.66, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.91, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54210 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 543.46, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.22, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 643.47, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54216 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 552.64, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.91, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54230 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 143, token usage: 0.00, cuda graph: True, gen throughput (token/s): 521.25, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 183, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.74, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 223, token usage: 0.00, cuda graph: True, gen throughput (token/s): 648.86, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54240 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 534.60, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 649.54, #queue-req: 0, \n[2025-11-06 12:11:06] INFO:     127.0.0.1:54250 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:06] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 528.40, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 652.97, #queue-req: 0, \n[2025-11-06 12:11:06] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 642.88, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54252 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 550.83, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.32, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54258 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 536.33, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.35, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.93, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54260 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 547.36, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.96, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54266 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 546.90, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.87, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.67, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54274 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.47, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.47, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:54290 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.85, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.60, #queue-req: 0, \n[2025-11-06 12:11:07] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.47, #queue-req: 0, \n[2025-11-06 12:11:07] INFO:     127.0.0.1:47364 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:07] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.97, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.45, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47374 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.93, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.60, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.17, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47378 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.67, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.08, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47382 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.86, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.14, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.30, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47394 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.54, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.69, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47400 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.08, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.53, #queue-req: 0, \n[2025-11-06 12:11:08] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.98, #queue-req: 0, \n[2025-11-06 12:11:08] INFO:     127.0.0.1:47410 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:08] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.58, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.17, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47418 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.94, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.76, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.94, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47428 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 560.48, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.01, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47436 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.42, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.54, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.81, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47440 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 545.39, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.36, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47446 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 549.87, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.12, #queue-req: 0, \n[2025-11-06 12:11:09] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.57, #queue-req: 0, \n[2025-11-06 12:11:09] INFO:     127.0.0.1:47458 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:09] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.93, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.82, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47472 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 553.31, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 659.21, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.62, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47474 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.44, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.08, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47476 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 90, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 144, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.05, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 184, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.35, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 224, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.83, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47484 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 164, token usage: 0.00, cuda graph: True, gen throughput (token/s): 560.44, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 204, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.17, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47492 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 557.54, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.34, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.10, #queue-req: 0, \n[2025-11-06 12:11:10] INFO:     127.0.0.1:47500 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:10] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:10] Decode batch. #running-req: 1, #token: 166, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.89, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 206, token usage: 0.00, cuda graph: True, gen throughput (token/s): 656.15, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47502 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 145, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.41, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 185, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.90, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 225, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.87, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47510 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.35, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.31, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47516 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.68, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.05, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 651.14, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47532 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 32, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 559.35, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.90, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47548 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 105, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 147, token usage: 0.00, cuda graph: True, gen throughput (token/s): 555.10, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 187, token usage: 0.00, cuda graph: True, gen throughput (token/s): 658.29, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 227, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.48, #queue-req: 0, \n[2025-11-06 12:11:11] INFO:     127.0.0.1:47560 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:11] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:11] Decode batch. #running-req: 1, #token: 165, token usage: 0.00, cuda graph: True, gen throughput (token/s): 558.23, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 205, token usage: 0.00, cuda graph: True, gen throughput (token/s): 654.64, #queue-req: 0, \n[2025-11-06 12:11:12] INFO:     127.0.0.1:47574 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:12] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 146, token usage: 0.00, cuda graph: True, gen throughput (token/s): 556.67, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 186, token usage: 0.00, cuda graph: True, gen throughput (token/s): 657.58, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 226, token usage: 0.00, cuda graph: True, gen throughput (token/s): 650.86, #queue-req: 0, \n[2025-11-06 12:11:12] INFO:     127.0.0.1:47584 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:12] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 167, token usage: 0.00, cuda graph: True, gen throughput (token/s): 561.84, #queue-req: 0, \n[2025-11-06 12:11:12] Decode batch. #running-req: 1, #token: 207, token usage: 0.00, cuda graph: True, gen throughput (token/s): 655.21, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47608 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 1, #new-token: 103, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47602 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47600 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47620 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 3, #new-token: 287, #cached-token: 111, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:13] Decode batch. #running-req: 4, #token: 486, token usage: 0.00, cuda graph: True, gen throughput (token/s): 51.16, #queue-req: 0, \n[2025-11-06 12:11:13] Decode batch. #running-req: 4, #token: 646, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2525.46, #queue-req: 0, \n[2025-11-06 12:11:13] Decode batch. #running-req: 4, #token: 806, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2482.07, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47662 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:13] INFO:     127.0.0.1:47646 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47630 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] INFO:     127.0.0.1:47670 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:13] Prefill batch. #new-seq: 3, #new-token: 300, #cached-token: 102, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 566, token usage: 0.00, cuda graph: True, gen throughput (token/s): 761.11, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 726, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2506.15, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47694 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 32, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47688 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47682 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47696 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 277, #cached-token: 122, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 479, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1722.99, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 639, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2521.97, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 799, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2480.97, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47730 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47724 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47712 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47742 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 2, #new-token: 175, #cached-token: 91, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 559, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1598.78, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 719, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2510.73, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47774 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47770 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47754 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47780 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 291, #cached-token: 109, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 475, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1715.38, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 635, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2525.14, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 795, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2483.72, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47800 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47794 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47788 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47812 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 309, #cached-token: 94, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 554, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1687.26, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 714, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2510.39, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47834 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 1, #new-token: 104, #cached-token: 30, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:14] INFO:     127.0.0.1:47822 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47818 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] INFO:     127.0.0.1:47850 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:14] Prefill batch. #new-seq: 3, #new-token: 299, #cached-token: 102, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:14] Decode batch. #running-req: 4, #token: 470, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1725.94, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 630, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2522.36, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 790, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2487.24, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47886 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47872 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47858 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 40, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47900 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 186, #cached-token: 79, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 89, #cached-token: 43, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 544, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1587.24, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 704, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2514.90, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47926 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 43, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47920 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47914 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47932 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 3, #new-token: 290, #cached-token: 110, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 466, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1673.98, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 626, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2523.37, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 786, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2486.53, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47954 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47944 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47938 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 96, #cached-token: 38, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47968 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 185, #cached-token: 81, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 102, #cached-token: 31, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 542, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1631.46, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 702, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2513.46, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47984 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47976 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:47974 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 93, #cached-token: 39, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:47988 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 184, #cached-token: 81, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 39, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 461, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1465.00, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 621, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2522.29, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 781, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2487.20, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:48022 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:48018 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] INFO:     127.0.0.1:48002 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 41, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:15] INFO:     127.0.0.1:48028 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:15] Prefill batch. #new-seq: 2, #new-token: 196, #cached-token: 71, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:15] Prefill batch. #new-seq: 1, #new-token: 92, #cached-token: 40, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:15] Decode batch. #running-req: 4, #token: 542, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1597.14, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 702, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2514.92, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48054 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48046 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48034 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 101, #cached-token: 31, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48058 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 2, #new-token: 182, #cached-token: 82, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 94, #cached-token: 40, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 449, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1582.60, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 609, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2524.42, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 769, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2489.38, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48076 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48072 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] INFO:     127.0.0.1:48070 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 91, #cached-token: 42, token usage: 0.00, #running-req: 0, #queue-req: 0, \n[2025-11-06 12:11:16] INFO:     127.0.0.1:48080 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n[2025-11-06 12:11:16] Prefill batch. #new-seq: 2, #new-token: 194, #cached-token: 70, token usage: 0.00, #running-req: 1, #queue-req: 0, \n[2025-11-06 12:11:16] Prefill batch. #new-seq: 1, #new-token: 95, #cached-token: 39, token usage: 0.00, #running-req: 3, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 541, token usage: 0.00, cuda graph: True, gen throughput (token/s): 1589.85, #queue-req: 0, \n[2025-11-06 12:11:16] Decode batch. #running-req: 4, #token: 701, token usage: 0.00, cuda graph: True, gen throughput (token/s): 2515.52, #queue-req: 0, \n",
      "objective_score": 0.18958974634150655
    }
  ]
}